{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "task1_20newsgroups.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo-6skzJJG0K"
      },
      "source": [
        "# Тематическая классификация длинных текстов - TFIDF и LogReg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1ezvFAfJG0K",
        "outputId": "2084d976-605e-466b-ad75-29127b8adae1"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/chmnk/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')\n",
        "\n",
        "import os\n",
        "print(os.listdir('.'))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 71, done.\u001b[K\n",
            "remote: Counting objects: 100% (71/71), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 337 (delta 40), reused 14 (delta 5), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (337/337), 42.38 MiB | 25.14 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.7.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 20.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/39/6f/86bd5d0eaa6821ba9193bbed16b660ea6f342fe63ec2e4fa2c61377bb44b/pyconll-2.3.3-py3-none-any.whl\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.5MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/08/1884157a3de72d41fa97cacacafaa49abf00eba53cb7e08615b2b65b4a9d/livelossplot-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (0.17.0)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 59.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 41.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (50.3.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (4.0.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.1.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (20.0.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.0.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.11.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (20.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\n",
            "Building wheels for collected packages: wget, ufal.udpipe, intervaltree\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9681 sha256=60e7c97381e3d10a43383b9b3f2b76cbba0e2fd6ddc5ea98cea1b637d52963c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625188 sha256=61391a126264ec3385f05f620fab43cfff93bce9bdc7d4835319919e2fec2311\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26101 sha256=3b3676e19b7d27a9ba27b7135fdfbc9048e5f206c6c0f4316b72948520f06305\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built wget ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts-ru, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll, gensim, wget, livelossplot\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pyconll-2.3.3 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n",
            "['.config', 'stepik-dl-nlp', 'sample_data']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:42:57.265628Z",
          "start_time": "2019-09-12T12:42:55.188211Z"
        },
        "id": "k6oMa3qJJG0L"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_text_simple_regex, tokenize_corpus, build_vocabulary, \\\n",
        "    vectorize_texts, SparseFeaturesDataset\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "import re\n",
        "import collections\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import scipy.sparse.linalg\n",
        "from copy import deepcopy\n",
        "\n",
        "import scipy.sparse\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYNyC8rXJG0L"
      },
      "source": [
        "## Предобработка текстов и подготовка признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:42:57.847399Z",
          "start_time": "2019-09-12T12:42:57.268037Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQqyaeEvJG0L",
        "outputId": "636d52cf-aa06-4c1f-d5f6-8dc5757c7c5f"
      },
      "source": [
        "train_source = fetch_20newsgroups(subset='train')\n",
        "test_source = fetch_20newsgroups(subset='test')\n",
        "\n",
        "print('Количество обучающих текстов', len(train_source['data']))\n",
        "print('Количество тестовых текстов', len(test_source['data']))\n",
        "print()\n",
        "print(train_source['data'][0].strip())\n",
        "\n",
        "print()\n",
        "print('Метка', train_source['target'][0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Количество обучающих текстов 11314\n",
            "Количество тестовых текстов 7532\n",
            "\n",
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n",
            "Lines: 15\n",
            "\n",
            " I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail.\n",
            "\n",
            "Thanks,\n",
            "- IL\n",
            "   ---- brought to you by your neighborhood Lerxst ----\n",
            "\n",
            "Метка 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWCeelePj4rP",
        "outputId": "9729232b-e42f-4559-edac-108604e53145"
      },
      "source": [
        "print(train_source.keys(), type(train_source))\n",
        "print(train_source['target_names'][7])\n",
        "print(' '.join(train_source['data'][0].split('\\n')[5:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR']) <class 'sklearn.utils.Bunch'>\n",
            "rec.autos\n",
            "  I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FJ_-4OqlhWQ"
      },
      "source": [
        "##Моя токенизация для мелких задач"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwZ6J-C4llTG",
        "outputId": "848ae873-234b-4c31-af41-efa004edf96a"
      },
      "source": [
        "TOKEN_RE_wd = re.compile(r'[\\w\\d]+')\n",
        "TOKEN_RE_mine = re.compile(r'[^\\d\\W]+|[\\d]*\\.[\\d]+|-[\\d]*\\.[\\d]+|-[\\d]+|[\\d]+|\\.[\\d]+|[^\\s]')\n",
        "\n",
        "\n",
        "def tokenize_text_simple_mine(txt, regex_, min_token_size=1):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = regex_.findall(txt)\n",
        "    return [token for token in all_tokens if len(token) >= min_token_size]\n",
        "\n",
        "txts = ['Мама мыла раму.',\n",
        "        'Демон123, как тебя зовут в реале?',\n",
        "        '-1-.15=-1.15',\n",
        "        '- 1 - .15=-1.15',\n",
        "        'Какого ;%:?* тут происходит?',\n",
        "        'Значение числа Е=2.7182.']\n",
        "\n",
        "for t in txts:\n",
        "    print(tokenize_text_simple_mine(t, regex_=TOKEN_RE_mine))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['мама', 'мыла', 'раму', '.']\n",
            "['демон', '123', ',', 'как', 'тебя', 'зовут', 'в', 'реале', '?']\n",
            "['-1', '-.15', '=', '-1.15']\n",
            "['-', '1', '-', '.15', '=', '-1.15']\n",
            "['какого', ';', '%', ':', '?', '*', 'тут', 'происходит', '?']\n",
            "['значение', 'числа', 'е', '=', '2.7182', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnEQxPzT1c-4",
        "outputId": "1bb06738-c3d5-4bc1-e469-0e111b0a38ca"
      },
      "source": [
        "def build_voc(tokenized):\n",
        "    word_counts = collections.defaultdict(int)\n",
        "    for txt_ in tokenized:\n",
        "        txt = set(txt_)\n",
        "        for t in txt:\n",
        "            word_counts[t] += 1\n",
        "    word_counts = sorted(word_counts.items(), key=lambda a: a[1])     \n",
        "    return word_counts\n",
        "\n",
        "def build_features(tokenized, voc):\n",
        "    ndocs = len(tokenized)\n",
        "    nw = len(voc)\n",
        "    w_to_id = {w[0]: i for i, w in enumerate(voc)}\n",
        "    tokenized_num = [[w_to_id[w] for w in tok] for tok in tokenized]\n",
        "    tok_feats = np.zeros((ndocs, nw))\n",
        "    for i, tok in enumerate(tokenized_num):\n",
        "        for j in tok:\n",
        "             tok_feats[i][j] += 1\n",
        "    print(tokenized_num)\n",
        "    print(tok_feats)\n",
        "    # print(np.sum(tok_feats, axis=1))\n",
        "    tf = tok_feats * (1. / np.expand_dims(np.sum(tok_feats, axis=1), axis=1))\n",
        "    idf = np.expand_dims(1. / np.sum(tok_feats, axis=0), axis=0)\n",
        "    ltfidf = np.log(tf + 1) * idf\n",
        "    feats_std = ltfidf.std(0, ddof=1)\n",
        "    feats_mean = ltfidf.mean(0)\n",
        "    print(feats_std, feats_mean)    \n",
        "    ltfidf -= feats_mean\n",
        "    ltfidf *= (1./feats_std)\n",
        "    for i in range(ndocs):\n",
        "        print(' '.join([str(k) for k in ltfidf[i]]))\n",
        "\n",
        "tokenized = tokenize_corpus(txts, min_token_size=1)\n",
        "\n",
        "txts = [\"Казнить нельзя, помиловать. Нельзя наказывать.\",\n",
        "        \"Казнить, нельзя помиловать. Нельзя освободить.\",\n",
        "        \"Нельзя не помиловать.\",\n",
        "        \"Обязательно освободить.\"\n",
        "        ]\n",
        "\n",
        "print(tokenized)\n",
        "tok_dict = build_voc(tokenized)\n",
        "print(tok_dict)\n",
        "print(' '.join([d[0] for d in tok_dict]))\n",
        "print(' '.join([str(d[1]/len(txts)) for d in tok_dict]))\n",
        "print(len(tok_dict))\n",
        "build_features(tokenized, tok_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['казнить', 'нельзя', 'помиловать', 'нельзя', 'наказывать'], ['казнить', 'нельзя', 'помиловать', 'нельзя', 'освободить'], ['нельзя', 'не', 'помиловать'], ['обязательно', 'освободить']]\n",
            "[('наказывать', 1), ('не', 1), ('обязательно', 1), ('казнить', 2), ('освободить', 2), ('нельзя', 3), ('помиловать', 3)]\n",
            "наказывать не обязательно казнить освободить нельзя помиловать\n",
            "0.25 0.25 0.25 0.5 0.5 0.75 0.75\n",
            "7\n",
            "[[3, 5, 6, 5, 0], [3, 5, 6, 5, 4], [5, 1, 6], [2, 4]]\n",
            "[[1. 0. 0. 1. 0. 2. 1.]\n",
            " [0. 0. 0. 1. 1. 2. 1.]\n",
            " [0. 1. 0. 0. 0. 1. 1.]\n",
            " [0. 0. 1. 0. 1. 0. 0.]]\n",
            "[0.09116078 0.14384104 0.20273255 0.0526317  0.09629374 0.0323496\n",
            " 0.03984286] [0.04558039 0.07192052 0.10136628 0.04558039 0.07347333 0.04803133\n",
            " 0.05436043]\n",
            "1.5 -0.5 -0.5 0.8660254037844387 -0.7630125578876946 0.5954669471484165 0.1609678767201035\n",
            "-0.5 -0.5 -0.5 0.8660254037844387 0.18368219185173848 0.5954669471484165 0.1609678767201035\n",
            "-0.5 1.5000000000000002 -0.5 -0.8660254037844387 -0.7630125578876946 0.2938239106183152 1.0424350837750262\n",
            "-0.5 -0.5 1.5 -0.8660254037844387 1.3423429239236506 -1.4847578049151486 -1.3643708372152332\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBRgCZWBJG0L"
      },
      "source": [
        "### Подготовка признаков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:00.294422Z",
          "start_time": "2019-09-12T12:42:57.849386Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWOObU8xJG0L",
        "outputId": "965bdad1-4eb4-401c-cc9f-037be362ecf0"
      },
      "source": [
        "train_tokenized = tokenize_corpus(train_source['data'])\n",
        "test_tokenized = tokenize_corpus(test_source['data'])\n",
        "\n",
        "print(' '.join(train_tokenized[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from lerxst where thing subject what this nntp posting host rac3 organization university maryland college park lines wondering anyone there could enlighten this other door sports looked from late early called bricklin doors were really small addition front bumper separate from rest body this know anyone tellme model name engine specs years production where this made history whatever info have this funky looking please mail thanks brought your neighborhood lerxst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:00.825372Z",
          "start_time": "2019-09-12T12:43:00.297392Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOZC6alJJG0L",
        "outputId": "92b6583b-5078-42c5-bb45-e94c2603b2cc"
      },
      "source": [
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 5\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
        "print(list(vocabulary.items())[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество уникальных токенов 21628\n",
            "[('that', 0), ('this', 1), ('have', 2), ('with', 3), ('writes', 4), ('article', 5), ('posting', 6), ('host', 7), ('nntp', 8), ('there', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:43:01.524600Z",
          "start_time": "2019-09-12T12:43:00.829107Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ejI2cEw2JG0L",
        "outputId": "9e43cd61-295a-431d-a898-d0e6cec831e3"
      },
      "source": [
        "plt.hist(word_doc_freq, bins=20)\n",
        "plt.title('Распределение относительных частот слов')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.094816Z",
          "start_time": "2019-09-12T12:43:01.526554Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu1sLNS4JG0M",
        "outputId": "bcf383de-22f7-452e-ae1e-f9ea13216f69"
      },
      "source": [
        "VECTORIZATION_MODE = 'tfidf'\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE)\n",
        "\n",
        "print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n",
        "print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n",
        "print()\n",
        "print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
        "print()\n",
        "print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Размерность матрицы признаков обучающей выборки (11314, 21628)\n",
            "Размерность матрицы признаков тестовой выборки (7532, 21628)\n",
            "\n",
            "Количество ненулевых элементов в обучающей выборке 1126792\n",
            "Процент заполненности матрицы признаков 0.46%\n",
            "\n",
            "Количество ненулевых элементов в тестовой выборке 721529\n",
            "Процент заполненности матрицы признаков 0.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.857114Z",
          "start_time": "2019-09-12T12:44:16.098773Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Ufi0ol-tJG0M",
        "outputId": "95e957e4-6bc6-4b3d-84ca-c828fd440728"
      },
      "source": [
        "plt.hist(train_vectors.data, bins=20)\n",
        "plt.title('Распределение весов признаков')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_lVOXbWJG0N"
      },
      "source": [
        "### Распределение классов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:16.864960Z",
          "start_time": "2019-09-12T12:44:16.859476Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5w0XYoLJG0N",
        "outputId": "3762ee86-a7ba-41eb-d16e-37d772caeeaa"
      },
      "source": [
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "print('Количество уникальных меток', UNIQUE_LABELS_N)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество уникальных меток 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.106036Z",
          "start_time": "2019-09-12T12:44:16.867310Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "QOqZOdYOJG0N",
        "outputId": "cc30c14e-25e5-4ba3-8585-42b77e4bb611"
      },
      "source": [
        "plt.hist(train_source['target'], bins=np.arange(0, 21))\n",
        "plt.title('Распределение меток в обучающей выборке');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.312198Z",
          "start_time": "2019-09-12T12:44:17.109884Z"
        },
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "sN0C8-1EJG0N",
        "outputId": "31de3f29-67b8-458d-d093-db4ec3d9d4be"
      },
      "source": [
        "plt.hist(test_source['target'], bins=np.arange(0, 21))\n",
        "plt.title('Распределение меток в тестовой выборке');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhFeSphpJG0N"
      },
      "source": [
        "### PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:44:17.319292Z",
          "start_time": "2019-09-12T12:44:17.315074Z"
        },
        "id": "aUSDBvGhJG0N"
      },
      "source": [
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFvIMLoIJG0N"
      },
      "source": [
        "## Обучение модели на PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:22.371272Z",
          "start_time": "2019-09-12T12:44:17.322178Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BTuDD1oJG0N",
        "outputId": "4d81108c-72b0-4e3e-8040-9b69045ab3f4"
      },
      "source": [
        "model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=5, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=1e-1,\n",
        "                                            epoch_n=200,\n",
        "                                            batch_size=32,\n",
        "                                            l2_reg_alpha=0,\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха 0\n",
            "Эпоха: 354 итераций, 2.04 сек\n",
            "Среднее значение функции потерь на обучении 2.2250226864033498\n",
            "Среднее значение функции потерь на валидации 2.1193228081121283\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 1\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.9164793370133739\n",
            "Среднее значение функции потерь на валидации 1.6757678445112907\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 2\n",
            "Эпоха: 354 итераций, 1.85 сек\n",
            "Среднее значение функции потерь на обучении 0.4659293549040617\n",
            "Среднее значение функции потерь на валидации 1.4736663820379872\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 3\n",
            "Эпоха: 354 итераций, 1.88 сек\n",
            "Среднее значение функции потерь на обучении 0.2840370426406968\n",
            "Среднее значение функции потерь на валидации 1.3451766247971584\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 4\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.19067820321537007\n",
            "Среднее значение функции потерь на валидации 1.2647168775231152\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 5\n",
            "Эпоха: 354 итераций, 1.85 сек\n",
            "Среднее значение функции потерь на обучении 0.13671450463181498\n",
            "Среднее значение функции потерь на валидации 1.2008045371306144\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 6\n",
            "Эпоха: 354 итераций, 1.84 сек\n",
            "Среднее значение функции потерь на обучении 0.10171837011341658\n",
            "Среднее значение функции потерь на валидации 1.1603826779430195\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 7\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.07843667881216033\n",
            "Среднее значение функции потерь на валидации 1.118864730000496\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 8\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.06154115289702254\n",
            "Среднее значение функции потерь на валидации 1.0848583114349235\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 9\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.049364450190342586\n",
            "Среднее значение функции потерь на валидации 1.0622101522098153\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 10\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.039994424348433984\n",
            "Среднее значение функции потерь на валидации 1.0450992958020355\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 11\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.032936409781397565\n",
            "Среднее значение функции потерь на валидации 1.0290353103714474\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 12\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.02756039533727292\n",
            "Среднее значение функции потерь на валидации 1.0156925421650127\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 13\n",
            "Эпоха: 354 итераций, 1.89 сек\n",
            "Среднее значение функции потерь на обучении 0.023192742129124828\n",
            "Среднее значение функции потерь на валидации 0.997167650673349\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 14\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.01949927708697058\n",
            "Среднее значение функции потерь на валидации 0.9858471587047739\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 15\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.01652250034040061\n",
            "Среднее значение функции потерь на валидации 0.9851063507593284\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 16\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.014381151896306855\n",
            "Среднее значение функции потерь на валидации 0.9818632644364389\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 17\n",
            "Эпоха: 354 итераций, 1.91 сек\n",
            "Среднее значение функции потерь на обучении 0.012216258887464634\n",
            "Среднее значение функции потерь на валидации 0.9786693655838401\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 18\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.010692148725239107\n",
            "Среднее значение функции потерь на валидации 0.9606715474845999\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 19\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.009467412650553604\n",
            "Среднее значение функции потерь на валидации 0.9558693073311094\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 20\n",
            "Эпоха: 354 итераций, 1.88 сек\n",
            "Среднее значение функции потерь на обучении 0.008242578644273897\n",
            "Среднее значение функции потерь на валидации 0.9674801797432414\n",
            "\n",
            "Эпоха 21\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.007372830275546149\n",
            "Среднее значение функции потерь на валидации 0.9419820119263762\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 22\n",
            "Эпоха: 354 итераций, 1.89 сек\n",
            "Среднее значение функции потерь на обучении 0.006707966396398316\n",
            "Среднее значение функции потерь на валидации 0.9431579471644709\n",
            "\n",
            "Эпоха 23\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.005953488514974376\n",
            "Среднее значение функции потерь на валидации 0.943088234607446\n",
            "\n",
            "Эпоха 24\n",
            "Эпоха: 354 итераций, 1.90 сек\n",
            "Среднее значение функции потерь на обучении 0.0055773390851991175\n",
            "Среднее значение функции потерь на валидации 0.9346266712172556\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 25\n",
            "Эпоха: 354 итераций, 1.89 сек\n",
            "Среднее значение функции потерь на обучении 0.004908504503375277\n",
            "Среднее значение функции потерь на валидации 0.9600751338116194\n",
            "\n",
            "Эпоха 26\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.004591085361698274\n",
            "Среднее значение функции потерь на валидации 0.9409770848134816\n",
            "\n",
            "Эпоха 27\n",
            "Эпоха: 354 итераций, 1.84 сек\n",
            "Среднее значение функции потерь на обучении 0.0044582263986832635\n",
            "Среднее значение функции потерь на валидации 0.9326977794200687\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 28\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.003890543268127946\n",
            "Среднее значение функции потерь на валидации 0.9328169679995311\n",
            "\n",
            "Эпоха 29\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.0037925839172557817\n",
            "Среднее значение функции потерь на валидации 0.9658883936324362\n",
            "\n",
            "Эпоха 30\n",
            "Эпоха: 354 итераций, 1.84 сек\n",
            "Среднее значение функции потерь на обучении 0.003105838056722798\n",
            "Среднее значение функции потерь на валидации 0.9778740503272768\n",
            "\n",
            "Эпоха 31\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.0033677035471915114\n",
            "Среднее значение функции потерь на валидации 0.9734006013910649\n",
            "\n",
            "Эпоха 32\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.0028430755388851297\n",
            "Среднее значение функции потерь на валидации 1.0043131933879044\n",
            "\n",
            "Эпоха 33\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.0029573843801360563\n",
            "Среднее значение функции потерь на валидации 0.9668052058856366\n",
            "Epoch    34: reducing learning rate of group 0 to 5.0000e-02.\n",
            "\n",
            "Эпоха 34\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.0023761899881791664\n",
            "Среднее значение функции потерь на валидации 0.9410562410445537\n",
            "\n",
            "Эпоха 35\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.00219726607997344\n",
            "Среднее значение функции потерь на валидации 0.9460452502056703\n",
            "\n",
            "Эпоха 36\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.002139114797084742\n",
            "Среднее значение функции потерь на валидации 0.9517218403013076\n",
            "\n",
            "Эпоха 37\n",
            "Эпоха: 354 итераций, 1.86 сек\n",
            "Среднее значение функции потерь на обучении 0.00213929154970328\n",
            "Среднее значение функции потерь на валидации 0.9406466273180509\n",
            "\n",
            "Эпоха 38\n",
            "Эпоха: 354 итераций, 1.87 сек\n",
            "Среднее значение функции потерь на обучении 0.0021328300663607464\n",
            "Среднее значение функции потерь на валидации 0.9461855736829466\n",
            "Модель не улучшилась за последние 10 эпох, прекращаем обучение\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D-T5ahuJG0N"
      },
      "source": [
        "## Оценка качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:25.105663Z",
          "start_time": "2019-09-12T12:46:22.373012Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jRJlfBLJG0N",
        "outputId": "3dbda323-6f21-432d-a352-7ec55b79b0b0"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),\n",
        "                             torch.from_numpy(train_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred),\n",
        "                            torch.from_numpy(test_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:01<00:00, 201.37it/s]\n",
            "  9%|▉         | 21/235.375 [00:00<00:01, 207.58it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.003156959777697921\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "236it [00:01, 205.16it/s]                             "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на валидации 0.9307225942611694\n",
            "Доля верных ответов 0.7651354221986192\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akvJlcIvJG0N"
      },
      "source": [
        "# Альтернативная реализация на scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:31.791405Z",
          "start_time": "2019-09-12T12:46:25.107897Z"
        },
        "scrolled": false,
        "id": "6p4BwIq2JG0N"
      },
      "source": [
        "sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
        "                                                      max_df=MAX_DF,\n",
        "                                                      min_df=MIN_COUNT)),\n",
        "                             ('cls', LogisticRegression())))\n",
        "sklearn_pipeline.fit(train_source['data'], train_source['target']);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hebxWdLcJG0N"
      },
      "source": [
        "## Оценка качества"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-09-12T12:46:35.454567Z",
          "start_time": "2019-09-12T12:46:31.792832Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6HM_mC9JG0N",
        "outputId": "185859af-5d4f-4505-dba8-02832d43e47d"
      },
      "source": [
        "sklearn_train_pred = sklearn_pipeline.predict_proba(train_source['data'])\n",
        "sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                 torch.from_numpy(train_source['target']))\n",
        "print('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "sklearn_test_pred = sklearn_pipeline.predict_proba(test_source['data'])\n",
        "sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                torch.from_numpy(test_source['target']))\n",
        "print('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 2.495478891857855\n",
            "Доля верных ответов 0.9716280714159449\n",
            "\n",
            "Среднее значение функции потерь на валидации 2.653902258233705\n",
            "Доля верных ответов 0.8190387679235263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FvSe1e85NwE"
      },
      "source": [
        "### Моя модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPCWwTrOCJ8P",
        "outputId": "f558da5d-c830-4c44-ef7c-2f446e076720"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('wordnet')\n",
        "lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "\n",
        "sno = nltk.stem.SnowballStemmer('english')\n",
        "\n",
        "stopwords_en = stopwords.words('english')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABDpdteBBDxF"
      },
      "source": [
        "TOKEN_RE = re.compile(r'[\\w\\d]+')\n",
        "\n",
        "\n",
        "def tokenize_text_simple_regex(txt, min_token_size=4, ngram=1):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    if ngram <= 1:\n",
        "        return [token for token in all_tokens if len(token) >= min_token_size]\n",
        "    else:\n",
        "        tokenized = [token for token in all_tokens if len(token) >= min_token_size]\n",
        "        return [tuple(tokenized[i: i + ngram]) for i in range(0, len(tokenized), ngram)]\n",
        "\n",
        "def tokenize_text_lemmatized(txt, min_token_size=4, ngram=1):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    lemmatized = [lemma.lemmatize(token) for token in all_tokens]\n",
        "    if ngram <= 1:\n",
        "        return list(filter(lambda x: len(x) >= min_token_size, lemmatized))\n",
        "    else:\n",
        "        lemmatized = list(filter(lambda x: len(x) >= min_token_size, lemmatized))\n",
        "        return [tuple(lemmatized[i: i + ngram]) for i in range(0, len(lemmatized), ngram)]\n",
        "\n",
        "\n",
        "def tokenize_text_stemmed(txt, min_token_size=4, ngram=1):\n",
        "    txt = txt.lower()\n",
        "    all_tokens = TOKEN_RE.findall(txt)\n",
        "    stemmed = [sno.stem(token) for token in all_tokens]\n",
        "    if ngram <= 1:\n",
        "        return list(filter(lambda x: len(x) >= min_token_size, stemmed))\n",
        "    else:\n",
        "        stemmed = list(filter(lambda x: len(x) >= min_token_size, stemmed))\n",
        "        return [tuple(stemmed[i: i + ngram]) for i in range(0, len(stemmed), ngram)]\n",
        "\n",
        "\n",
        "# def character_tokenize(txt):\n",
        "#     return list(txt)\n",
        "\n",
        "\n",
        "# def tokenize_corpus(texts, tokenizer=tokenize_text_simple_regex, **tokenizer_kwargs):\n",
        "#     return [tokenizer(text, **tokenizer_kwargs) for text in texts]\n",
        "\n",
        "\n",
        "# def add_fake_token(word2id, token='<PAD>'):\n",
        "#     word2id_new = {token: i + 1 for token, i in word2id.items()}\n",
        "#     word2id_new[token] = 0\n",
        "#     return word2id_new\n",
        "\n",
        "\n",
        "# def texts_to_token_ids(tokenized_texts, word2id):\n",
        "#     return [[word2id[token] for token in text if token in word2id]\n",
        "#             for text in tokenized_texts]\n",
        "\n",
        "\n",
        "def build_vocabulary(tokenized_texts, max_size=1000000, max_doc_freq=0.8, min_count=5, pad_word=None, use_pmi_filter=False, pmi_voc_prev=None, pmi_wl_matrix=None):\n",
        "    word_counts = collections.defaultdict(int)\n",
        "    doc_n = 0\n",
        "\n",
        "    # посчитать количество документов, в которых употребляется каждое слово\n",
        "    # а также общее количество документов\n",
        "    for txt in tokenized_texts:\n",
        "        doc_n += 1\n",
        "        unique_text_tokens = set(txt)\n",
        "        for token in unique_text_tokens:\n",
        "            word_counts[token] += 1\n",
        "    \n",
        "    # убрать слишком редкие и слишком частые слова\n",
        "    \n",
        "    if use_pmi_filter and pmi_wl_matrix is not None and pmi_voc_prev is not None:\n",
        "        # pmi_std_bad = (np.std(pmi_wl_matrix, axis=1) >= 1.5)\n",
        "        pmi_median_bad = (np.median(pmi_wl_matrix, axis=1) >= 0.5)\n",
        "        word_counts = {word: cnt for word, cnt in word_counts.items()\n",
        "                       if cnt >= min_count and cnt / doc_n <= max_doc_freq \n",
        "                       and pmi_median_bad[pmi_voc_prev[word]]}\n",
        "    else:\n",
        "        word_counts = {word: cnt for word, cnt in word_counts.items()\n",
        "                       if cnt >= min_count and cnt / doc_n <= max_doc_freq}\n",
        "\n",
        "    # отсортировать слова по убыванию частоты\n",
        "    sorted_word_counts = sorted(word_counts.items(),\n",
        "                                reverse=True,\n",
        "                                key=lambda pair: pair[1])\n",
        "\n",
        "    # добавим несуществующее слово с индексом 0 для удобства пакетной обработки\n",
        "    if pad_word is not None:\n",
        "        sorted_word_counts = [(pad_word, 0)] + sorted_word_counts\n",
        "\n",
        "    # если у нас по прежнему слишком много слов, оставить только max_size самых частотных\n",
        "    if len(word_counts) > max_size:\n",
        "        sorted_word_counts = sorted_word_counts[:max_size]\n",
        "\n",
        "    # нумеруем слова\n",
        "    word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n",
        "\n",
        "    # # нормируем частоты слов\n",
        "    word2freq = np.array([cnt  for _, cnt in sorted_word_counts], dtype='float32')  # cnt / doc_n\n",
        "\n",
        "    return word2id, word2freq\n",
        "\n",
        "\n",
        "def vectorize_texts(tokenized_texts, word2id, word2freq, mode='tfidf', scale=None, pmi_wl_matrix=None, scalers=None):\n",
        "    assert mode in {'tfidf', 'idf', 'tf', 'bin', 'ltfidf', 'ltfpmi', 'tfidfnew', 'ltfidfnew'}\n",
        "    assert scale in {'minmax', 'rownorm', 'std', None}\n",
        "    n_docs = len(tokenized_texts)\n",
        "    # считаем количество употреблений каждого слова в каждом документе\n",
        "    result = scipy.sparse.dok_matrix((len(tokenized_texts), len(word2id)), dtype='float32')\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                result[text_i, word2id[token]] += 1\n",
        "\n",
        "    # получаем бинарные вектора \"встречается или нет\"\n",
        "    if mode == 'bin':\n",
        "        result = (result > 0).astype('float32')\n",
        "\n",
        "    # получаем вектора относительных частот слова в документе\n",
        "    elif mode == 'tf':\n",
        "        result = result.tocsr()\n",
        "        # result = result.multiply(1 / result.sum(1))\n",
        "\n",
        "    # полностью убираем информацию о количестве употреблений слова в данном документе,\n",
        "    # но оставляем информацию о частотности слова в корпусе в целом\n",
        "    elif mode == 'idf':\n",
        "        result = (result > 0).astype('float32').multiply(n_docs / word2freq)\n",
        "\n",
        "    # учитываем всю информацию, которая у нас есть:\n",
        "    # частоту слова в документе и частоту слова в корпусе\n",
        "    elif mode == 'tfidf':\n",
        "        result = result.tocsr()\n",
        "        result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
        "        result = result.multiply(n_docs / word2freq)  # разделить каждый столбец на вес слова\n",
        "        \n",
        "    elif mode == 'ltfidf':\n",
        "        result = (result.multiply(1 / result.sum(1))).log1p()\n",
        "        result = result.multiply(n_docs / word2freq)\n",
        "\n",
        "    elif mode == 'ltfpmi':\n",
        "        result = (result.multiply(1 / result.sum(1))).log1p()\n",
        "        result = result.multiply(np.amax(pmi_wl_matrix, axis=0))\n",
        "\n",
        "    elif mode == 'tfidfnew':\n",
        "        result = result.tocsr()\n",
        "        idf = np.log((n_docs + 1)/ (word2freq + 1)) + 1\n",
        "        # result = result.multiply(1 / result.sum(1))  # разделить каждую строку на её длину\n",
        "        result = result.multiply(idf)  # разделить каждый столбец на вес слова\n",
        "        \n",
        "    elif mode == 'ltfidfnew':\n",
        "        result = result.tocsr()\n",
        "        idf = np.log((n_docs + 1)/ (word2freq + 1)) + 1\n",
        "        # result = (result.multiply(1 / result.sum(1))).log1p()\n",
        "        result = result.log1p()\n",
        "        result = result.multiply(idf)\n",
        "  \n",
        "    if scale == 'std':\n",
        "        result = result.tocsc()\n",
        "        if scalers is None:\n",
        "            rstd = 1. / result.A.std(axis=0)\n",
        "            result = result.multiply(rstd)\n",
        "            return result.tocsr(), None, rstd\n",
        "        else:\n",
        "            result = result.multiply(scalers[1])\n",
        "    elif scale == 'minmax':\n",
        "        result = result.tocsc()\n",
        "        if scalers is None:\n",
        "            rmin = result.min()\n",
        "            result -= rmin\n",
        "            rmax = 1. / (result.max() + 1e-6)\n",
        "            result = result.multiply(rmax)\n",
        "            return result.tocsr(), rmin, rmax\n",
        "        else:\n",
        "          result -= scalers[0]\n",
        "          result = result.multiply(scalers[1])\n",
        "    elif scale == 'rownorm':\n",
        "        result = result.tocsr()\n",
        "        rown = np.expand_dims(1. / scipy.sparse.linalg.norm(result, axis=1), axis=1)\n",
        "        result = result.multiply(rown)\n",
        "        \n",
        "    return result.tocsr()\n",
        "\n",
        "\n",
        "\n",
        "def compute_pmi_wl_matrix(tokenized_texts, word2id, labels, naive=False):\n",
        "    # train_source['data']\n",
        "    nc = len(tokenized_texts)\n",
        "    nl = len(set(labels))\n",
        "    nlall = len(labels)\n",
        "    nw = len(word2id)\n",
        "    wl_matrix = np.zeros((nw, nl))\n",
        "    pmi_wl_matrix = np.zeros((nw, nl))\n",
        "    for text_i, text in enumerate(tokenized_texts):\n",
        "        for token in text:\n",
        "            if token in word2id:\n",
        "                wl_matrix[word2id[token]][labels[text_i]] = 1.\n",
        "    # p(w,l) = n_docs_with_w_and_l / n_docs\n",
        "    # p(w) = n_docs_with_w / n_docs\n",
        "    # p(l) = n_docs_with_l / n_docs\n",
        "    # p(w, l) / p(w) / p(l) = n_docs_with_w_and_l * n_docs / n_docs_with_w / n_docs_with_l\n",
        "    if naive:\n",
        "        pw = np.zeros(nw)\n",
        "        for i in range(nw):\n",
        "            for j in range(nl):\n",
        "                pw[i] += wl_matrix[i][j]\n",
        "        pl = np.zeros(nl)\n",
        "        for i in range(nlall):\n",
        "            pl[labels[i]] += 1.\n",
        "        for i in range(nw):\n",
        "            for j in range(nl):\n",
        "                pmi_wl_matrix[i][j] = np.log1p(wl_matrix[i][j] * nlall/pw[i]/pl[j])\n",
        "        pmi_wl_matrix[np.where(pmi_wl_matrix < 0)] = 0\n",
        "        return pmi_wl_matrix\n",
        "    return None\n",
        "       \n",
        "    \n",
        "\n",
        "# PAD_TOKEN = '__PAD__'\n",
        "# NUMERIC_TOKEN = '__NUMBER__'\n",
        "# NUMERIC_RE = re.compile(r'^([0-9.,e+\\-]+|[mcxvi]+)$', re.I)\n",
        "\n",
        "\n",
        "# def replace_number_nokens(tokenized_texts):\n",
        "#     return [[token if not NUMERIC_RE.match(token) else NUMERIC_TOKEN for token in text]\n",
        "#             for text in tokenized_texts]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeMYPEzOe4LN",
        "outputId": "ded62103-8c74-4df2-d5c4-a751a70f092a"
      },
      "source": [
        "print(np.shape(np.expand_dims(np.array([1, 2, 3]), axis=1)))\n",
        "print(np.sum(np.ones((10, 5)), axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 1)\n",
            "[10. 10. 10. 10. 10.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5R_Az7CAsrS",
        "outputId": "b87d53c1-b150-48e1-f407-b205225025e3"
      },
      "source": [
        "train_tokenized = tokenize_corpus(train_source['data'], tokenizer=tokenize_text_lemmatized, min_token_size=4)\n",
        "test_tokenized = tokenize_corpus(test_source['data'], tokenizer=tokenize_text_lemmatized, min_token_size=4)\n",
        "print(' '.join(train_tokenized[0]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from lerxst where thing subject what this nntp posting host rac3 organization university maryland college park line wondering anyone there could enlighten this other door sport looked from late early called bricklin door were really small addition front bumper separate from rest body this know anyone tellme model name engine spec year production where this made history whatever info have this funky looking please mail thanks brought your neighborhood lerxst\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFRJXjKFA0Pl",
        "outputId": "ae1a8183-04dd-446a-8dd6-eceffc8a0178"
      },
      "source": [
        "MAX_DF = 0.8\n",
        "MIN_COUNT = 6\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT)\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
        "print(list(vocabulary.items())[:10])\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество уникальных токенов 16858\n",
            "[('that', 0), ('this', 1), ('have', 2), ('with', 3), ('writes', 4), ('article', 5), ('posting', 6), ('host', 7), ('nntp', 8), ('there', 9)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0hyR9j2xN-1",
        "outputId": "5ab0b60b-95f9-4ce6-96ac-fa14c8d1a0d3"
      },
      "source": [
        "UNIQUE_LABELS_N = len(set(train_source['target']))\n",
        "print('Количество уникальных меток', UNIQUE_LABELS_N)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество уникальных меток 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtP7QKfLfeHj"
      },
      "source": [
        "pmi_wl_matrix_naive = compute_pmi_wl_matrix(train_tokenized, vocabulary, train_source['target'], naive=True)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hpg2MgPvcuM",
        "outputId": "15adc8eb-b738-4cd2-8bec-d6697d21a58d"
      },
      "source": [
        "# print(pmi_wl_matrix[0, :])\n",
        "print(pmi_wl_matrix_naive[0, :])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77865569 0.67735533 0.67151038 0.67233896 0.68245003 0.66985957\n",
            " 0.67651388 0.66903731 0.66576911 0.66658305 0.66414739 0.66821714\n",
            " 0.67151038 0.66903731 0.66985957 0.66495722 0.71102668 0.69465314\n",
            " 0.79595606 0.91650291]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "E5sbNbsPlyNO",
        "outputId": "19ad0017-f22a-4116-fd88-8447acb9c82f"
      },
      "source": [
        "print(pmi_wl_matrix_naive[0, :])\n",
        "print(pmi_wl_matrix_naive[10, :])\n",
        "print(pmi_wl_matrix_naive[920, :], [k for k, v in vocabulary.items() if v == 920])\n",
        "plt.hist(np.amax(pmi_wl_matrix_naive, axis=1))\n",
        "# plt.hist(pmi_wl_matrix_naive)\n",
        "# print(pmi_wl_matrix_naive.shape)\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.77865569 0.67735533 0.67151038 0.67233896 0.68245003 0.66985957\n",
            " 0.67651388 0.66903731 0.66576911 0.66658305 0.66414739 0.66821714\n",
            " 0.67151038 0.66903731 0.66985957 0.66495722 0.71102668 0.69465314\n",
            " 0.79595606 0.91650291]\n",
            "[0.77865569 0.67735533 0.67151038 0.67233896 0.68245003 0.66985957\n",
            " 0.67651388 0.66903731 0.66576911 0.66658305 0.66414739 0.66821714\n",
            " 0.67151038 0.66903731 0.66985957 0.66495722 0.71102668 0.69465314\n",
            " 0.79595606 0.91650291]\n",
            "[0.77865569 0.67735533 0.67151038 0.67233896 0.68245003 0.66985957\n",
            " 0.67651388 0.66903731 0.66576911 0.66658305 0.66414739 0.66821714\n",
            " 0.67151038 0.66903731 0.66985957 0.66495722 0.71102668 0.69465314\n",
            " 0.79595606 0.91650291] ['trade']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2759., 2501., 2459., 2914., 1881., 1519.,  748.,  162., 1784.,\n",
              "         131.]),\n",
              " array([0.79701984, 1.0607508 , 1.32448175, 1.58821271, 1.85194367,\n",
              "        2.11567462, 2.37940558, 2.64313654, 2.90686749, 3.17059845,\n",
              "        3.43432941]),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruMDWFrHxXCM"
      },
      "source": [
        "Фильтруем словарь второй раз с PMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "-0fKB7wtxV1k",
        "outputId": "9f728ef2-e15e-4cad-f6ba-29d25c217b74"
      },
      "source": [
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=MAX_DF, min_count=MIN_COUNT, use_pmi_filter=True, pmi_voc_prev=vocabulary, pmi_wl_matrix=pmi_wl_matrix_naive)\n",
        "UNIQUE_WORDS_N = len(vocabulary)\n",
        "print('Количество уникальных токенов', UNIQUE_WORDS_N)\n",
        "print(list(vocabulary.items())[:10])\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-5da8d35f71d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_doc_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tokenized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_DF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMIN_COUNT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_pmi_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmi_voc_prev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpmi_wl_matrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpmi_wl_matrix_naive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mUNIQUE_WORDS_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Количество уникальных токенов'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUNIQUE_WORDS_N\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pmi_wl_matrix_naive' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "8ZqByzGSCaRt",
        "outputId": "31cc7f32-6c7b-4b6b-ba93-e66a16915afe"
      },
      "source": [
        "plt.hist(word_doc_freq, bins=20)\n",
        "plt.title('Распределение относительных частот слов')\n",
        "plt.yscale('log');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6gRRoE0A7Lz",
        "outputId": "54d2470a-be35-4343-8b96-93d8928451b7"
      },
      "source": [
        "VECTORIZATION_MODE = 'ltfidfnew'\n",
        "\n",
        "train_vectors = vectorize_texts(train_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE, scale=None)\n",
        "test_vectors = vectorize_texts(test_tokenized, vocabulary, word_doc_freq, mode=VECTORIZATION_MODE, scale=None)\n",
        "\n",
        "print('Размерность матрицы признаков обучающей выборки', train_vectors.shape)\n",
        "print('Размерность матрицы признаков тестовой выборки', test_vectors.shape)\n",
        "print()\n",
        "print('Количество ненулевых элементов в обучающей выборке', train_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors.nnz * 100 / (train_vectors.shape[0] * train_vectors.shape[1])))\n",
        "print()\n",
        "print('Количество ненулевых элементов в тестовой выборке', test_vectors.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors.nnz * 100 / (test_vectors.shape[0] * test_vectors.shape[1])))\n",
        "\n",
        "sc = Normalizer()\n",
        "train_vectors= sc.fit_transform(train_vectors)\n",
        "test_vectors = sc.transform(test_vectors)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Размерность матрицы признаков обучающей выборки (11314, 16858)\n",
            "Размерность матрицы признаков тестовой выборки (7532, 16858)\n",
            "\n",
            "Количество ненулевых элементов в обучающей выборке 1078068\n",
            "Процент заполненности матрицы признаков 0.57%\n",
            "\n",
            "Количество ненулевых элементов в тестовой выборке 693058\n",
            "Процент заполненности матрицы признаков 0.55%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oe5PEpXMFScL",
        "outputId": "a6fb1d75-a691-4c24-841b-f78c331385fe"
      },
      "source": [
        "vectorizer = TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
        "                             max_df=MAX_DF,\n",
        "                             min_df=MIN_COUNT,\n",
        "                             sublinear_tf=True, norm='l2')\n",
        "train_vectors_sklearn = vectorizer.fit_transform(train_source['data'])\n",
        "test_vectors_sklearn = vectorizer.transform(test_source['data'])\n",
        "\n",
        "print('Размерность матрицы признаков обучающей выборки', train_vectors_sklearn.shape)\n",
        "print('Размерность матрицы признаков тестовой выборки', test_vectors_sklearn.shape)\n",
        "print()\n",
        "print('Количество ненулевых элементов в обучающей выборке', train_vectors_sklearn.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(train_vectors_sklearn.nnz * 100 / (train_vectors_sklearn.shape[0] * train_vectors_sklearn.shape[1])))\n",
        "print()\n",
        "print('Количество ненулевых элементов в тестовой выборке', test_vectors_sklearn.nnz)\n",
        "print('Процент заполненности матрицы признаков {:.2f}%'.format(test_vectors_sklearn.nnz * 100 / (test_vectors_sklearn.shape[0] * test_vectors_sklearn.shape[1])))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Размерность матрицы признаков обучающей выборки (11314, 18836)\n",
            "Размерность матрицы признаков тестовой выборки (7532, 18836)\n",
            "\n",
            "Количество ненулевых элементов в обучающей выборке 1112832\n",
            "Процент заполненности матрицы признаков 0.52%\n",
            "\n",
            "Количество ненулевых элементов в тестовой выборке 714090\n",
            "Процент заполненности матрицы признаков 0.50%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtBBu3KR1G-Y"
      },
      "source": [
        "train_vectors_new = deepcopy(train_vectors)"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD93Dt3GI3p9"
      },
      "source": [
        "sc = MaxAbsScaler()\n",
        "train_vectors_new = sc.fit_transform(train_vectors_new)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehykX-WnTYeF"
      },
      "source": [
        "sc = StandardScaler(with_mean=False)\n",
        "train_vectors_new = sc.fit_transform(train_vectors_new)"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMsa9RJSKZlT"
      },
      "source": [
        "sc = Normalizer()\n",
        "train_vectors_new = sc.fit_transform(train_vectors_new)\n",
        "# train_vectors= sc.fit_transform(train_vectors)\n",
        "# test_vectors = sc.transform(test_vectors)"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4vSDW3-Nznc",
        "outputId": "42712c44-31db-4c2b-aa4f-5863764b7b28"
      },
      "source": [
        "print(max(train_vectors_sklearn.A[train_vectors_sklearn[0].nonzero()]))\n",
        "print(max(train_vectors_new.A[train_vectors_new[0].nonzero()]))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.25651549389185374\n",
            "0.13979957\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqYsTeXVODzq",
        "outputId": "0c048ec6-7107-4d17-8bd0-fc5376551f1d"
      },
      "source": [
        "print(sorted(train_vectors_sklearn.A[train_vectors_sklearn[0].nonzero()]))\n",
        "print(sorted(train_vectors_new.A[train_vectors_new[0].nonzero()]))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.04663859412029696, 0.05502931670199472, 0.05654514058182023, 0.056945733563097664, 0.058616254243042586, 0.05883333653589512, 0.05920319375135247, 0.06830096528937336, 0.06833810110400604, 0.07262939910329873, 0.07892096608496889, 0.07905255552893245, 0.08797800230918744, 0.08853031649278313, 0.09045967620995454, 0.09562068375350927, 0.09677570546139458, 0.09919998552687555, 0.10120085546020739, 0.11029015881549512, 0.11029015881549512, 0.11175294193440444, 0.11675011966966567, 0.1201435663358021, 0.12036447260719725, 0.12453922942449437, 0.1304434492723441, 0.13101507973968946, 0.1331816970170127, 0.13493423240344818, 0.1380381004285365, 0.13891007602352246, 0.1391469109336612, 0.14118966738168004, 0.14179928237706385, 0.14425564862699294, 0.14594036152690357, 0.14717036160217317, 0.14932397489773838, 0.15221244905570008, 0.15508391714091357, 0.15755815053158698, 0.15825321089612424, 0.15987738049972725, 0.17255633024469844, 0.17283818790436536, 0.17312266753339067, 0.17458616876855068, 0.18523785406816887, 0.21003718639577584, 0.21201086082745585, 0.21412075525983243, 0.2488299620162008, 0.25651549389185374]\n",
            "[0.0467607, 0.05517388, 0.056693763, 0.057095427, 0.05877042, 0.05898808, 0.05935893, 0.06848105, 0.06851829, 0.07282107, 0.079129465, 0.079261415, 0.08821075, 0.08876453, 0.09069907, 0.095873885, 0.097031996, 0.09946276, 0.10146899, 0.11058262, 0.11058262, 0.11204931, 0.11705987, 0.119547606, 0.120462395, 0.12486983, 0.13037805, 0.13078983, 0.13136299, 0.13353541, 0.13529265, 0.13697673, 0.13840482, 0.13951659, 0.14156485, 0.14217606, 0.144639, 0.14756152, 0.1497209, 0.15261711, 0.15549625, 0.15797712, 0.15867403, 0.16030255, 0.17301543, 0.17329806, 0.17358328, 0.17505069, 0.18573089, 0.2105966, 0.21257555, 0.2146911, 0.24949321, 0.25719932]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "RyD8TavQNHp2",
        "outputId": "4b8113a4-ae5d-4960-c3c9-4a494d104fd7"
      },
      "source": [
        "plt.hist(train_vectors_sklearn.A[train_vectors_sklearn[0].nonzero()])\n",
        "plt.hist(train_vectors_new.A[train_vectors_new[0].nonzero()], alpha=0.8)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 7.,  7.,  5.,  9., 11.,  6.,  4.,  3.,  0.,  2.]),\n",
              " array([0.0467607 , 0.06780456, 0.08884843, 0.10989229, 0.13093615,\n",
              "        0.15198001, 0.17302386, 0.19406773, 0.2151116 , 0.23615545,\n",
              "        0.25719932], dtype=float32),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbNO-hgE7ZFx"
      },
      "source": [
        "Отредактированное dlnlputils/pipeline.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RfkwtIEJG0N"
      },
      "source": [
        "import copy\n",
        "import datetime\n",
        "import random\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def init_random_seed(value=0):\n",
        "    random.seed(value)\n",
        "    np.random.seed(value)\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def copy_data_to_device(data, device):\n",
        "    if torch.is_tensor(data):\n",
        "        return data.to(device)\n",
        "    elif isinstance(data, (list, tuple)):\n",
        "        return [copy_data_to_device(elem, device) for elem in data]\n",
        "    raise ValueError('Недопустимый тип данных {}'.format(type(data)))\n",
        "\n",
        "\n",
        "def print_grad_stats(model):\n",
        "    mean = 0\n",
        "    std = 0\n",
        "    norm = 1e-5\n",
        "    for param in model.parameters():\n",
        "        grad = getattr(param, 'grad', None)\n",
        "        if grad is not None:\n",
        "            mean += grad.data.abs().mean()\n",
        "            std += grad.data.std()\n",
        "            norm += 1\n",
        "    mean /= norm\n",
        "    std /= norm\n",
        "    print(f'Mean grad {mean}, std {std}, n {norm}')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1xUNT1zEscD"
      },
      "source": [
        "def train_eval_loop(model, train_dataset, val_dataset, criterion,\n",
        "                    lr=1e-4, epoch_n=10, batch_size=32,\n",
        "                    device=None, early_stopping_patience=10, l2_reg_alpha=0,\n",
        "                    max_batches_per_epoch_train=10000,\n",
        "                    max_batches_per_epoch_val=1000,\n",
        "                    data_loader_ctor=DataLoader,\n",
        "                    optimizer_ctor=None,\n",
        "                    lr_scheduler_ctor=None,\n",
        "                    shuffle_train=True,\n",
        "                    dataloader_workers_n=0):\n",
        "    \"\"\"\n",
        "    Цикл для обучения модели. После каждой эпохи качество модели оценивается по отложенной выборке.\n",
        "    :param model: torch.nn.Module - обучаемая модель\n",
        "    :param train_dataset: torch.utils.data.Dataset - данные для обучения\n",
        "    :param val_dataset: torch.utils.data.Dataset - данные для оценки качества\n",
        "    :param criterion: функция потерь для настройки модели\n",
        "    :param lr: скорость обучения\n",
        "    :param epoch_n: максимальное количество эпох\n",
        "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
        "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
        "    :param early_stopping_patience: наибольшее количество эпох, в течение которых допускается\n",
        "        отсутствие улучшения модели, чтобы обучение продолжалось.\n",
        "    :param l2_reg_alpha: коэффициент L2-регуляризации\n",
        "    :param max_batches_per_epoch_train: максимальное количество итераций на одну эпоху обучения\n",
        "    :param max_batches_per_epoch_val: максимальное количество итераций на одну эпоху валидации\n",
        "    :param data_loader_ctor: функция для создания объекта, преобразующего датасет в батчи\n",
        "        (по умолчанию torch.utils.data.DataLoader)\n",
        "    :return: кортеж из двух элементов:\n",
        "        - среднее значение функции потерь на валидации на лучшей эпохе\n",
        "        - лучшая модель\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "\n",
        "    if optimizer_ctor is None:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n",
        "    else:\n",
        "        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n",
        "\n",
        "    if lr_scheduler_ctor is not None:\n",
        "        lr_scheduler = lr_scheduler_ctor(optimizer)\n",
        "    else:\n",
        "        lr_scheduler = None\n",
        "\n",
        "    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n",
        "                                        num_workers=dataloader_workers_n)\n",
        "    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                                      num_workers=dataloader_workers_n)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch_i = 0\n",
        "    best_model = copy.deepcopy(model)\n",
        "\n",
        "    for epoch_i in range(epoch_n):\n",
        "        try:\n",
        "            epoch_start = datetime.datetime.now()\n",
        "            print('Эпоха {}'.format(epoch_i))\n",
        "\n",
        "            model.train()\n",
        "            mean_train_loss = 0\n",
        "            train_batches_n = 0\n",
        "            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n",
        "                if batch_i > max_batches_per_epoch_train:\n",
        "                    break\n",
        "\n",
        "                batch_x = copy_data_to_device(batch_x, device)\n",
        "                batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                pred = model(batch_x)\n",
        "                loss = criterion(pred, batch_y)\n",
        "\n",
        "                model.zero_grad()\n",
        "                loss.backward()\n",
        "\n",
        "                optimizer.step()\n",
        "\n",
        "                mean_train_loss += float(loss)\n",
        "                train_batches_n += 1\n",
        "\n",
        "            mean_train_loss /= train_batches_n\n",
        "            print('Эпоха: {} итераций, {:0.2f} сек'.format(train_batches_n,\n",
        "                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n",
        "            print('Среднее значение функции потерь на обучении', mean_train_loss)\n",
        "\n",
        "\n",
        "\n",
        "            model.eval()\n",
        "            mean_val_loss = 0\n",
        "            val_batches_n = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n",
        "                    if batch_i > max_batches_per_epoch_val:\n",
        "                        break\n",
        "\n",
        "                    batch_x = copy_data_to_device(batch_x, device)\n",
        "                    batch_y = copy_data_to_device(batch_y, device)\n",
        "\n",
        "                    pred = model(batch_x)\n",
        "                    loss = criterion(pred, batch_y)\n",
        "\n",
        "                    mean_val_loss += float(loss)\n",
        "                    val_batches_n += 1\n",
        "\n",
        "            mean_val_loss /= val_batches_n\n",
        "            print('Среднее значение функции потерь на валидации', mean_val_loss)\n",
        "\n",
        "            if mean_val_loss < best_val_loss:\n",
        "                best_epoch_i = epoch_i\n",
        "                best_val_loss = mean_val_loss\n",
        "                best_model = copy.deepcopy(model)\n",
        "                print('Новая лучшая модель!')\n",
        "            elif epoch_i - best_epoch_i > early_stopping_patience:\n",
        "                print('Модель не улучшилась за последние {} эпох, прекращаем обучение'.format(\n",
        "                    early_stopping_patience))\n",
        "                break\n",
        "\n",
        "            if lr_scheduler is not None:\n",
        "                lr_scheduler.step(mean_val_loss)\n",
        "\n",
        "            print()\n",
        "        except KeyboardInterrupt:\n",
        "            print('Досрочно остановлено пользователем')\n",
        "            break\n",
        "        except Exception as ex:\n",
        "            print('Ошибка при обучении: {}\\n{}'.format(ex, traceback.format_exc()))\n",
        "            break\n",
        "\n",
        "    return best_val_loss, best_model\n",
        "\n",
        "\n",
        "def predict_with_model(model, dataset, device=None, batch_size=32, num_workers=0, return_labels=False):\n",
        "    \"\"\"\n",
        "    :param model: torch.nn.Module - обученная модель\n",
        "    :param dataset: torch.utils.data.Dataset - данные для применения модели\n",
        "    :param device: cuda/cpu - устройство, на котором выполнять вычисления\n",
        "    :param batch_size: количество примеров, обрабатываемых моделью за одну итерацию\n",
        "    :return: numpy.array размерности len(dataset) x *\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    results_by_batch = []\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        import tqdm\n",
        "        for batch_x, batch_y in tqdm.tqdm(dataloader, total=len(dataset)/batch_size):\n",
        "            batch_x = copy_data_to_device(batch_x, device)\n",
        "\n",
        "            if return_labels:\n",
        "                labels.append(batch_y.numpy())\n",
        "\n",
        "            batch_pred = model(batch_x)\n",
        "            results_by_batch.append(batch_pred.detach().cpu().numpy())\n",
        "\n",
        "    if return_labels:\n",
        "        return np.concatenate(results_by_batch, 0), np.concatenate(labels, 0)\n",
        "    else:\n",
        "        return np.concatenate(results_by_batch, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K90Tz6Xp7kOH"
      },
      "source": [
        "UNIQUE_WORDS_N = train_vectors.shape[1]\n",
        "train_dataset = SparseFeaturesDataset(train_vectors, train_source['target'])\n",
        "test_dataset = SparseFeaturesDataset(test_vectors, test_source['target'])"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFj8QS0n7rzc",
        "outputId": "eb317998-ac5c-468a-c13c-e4291005e53d"
      },
      "source": [
        "# model = nn.Linear(UNIQUE_WORDS_N, UNIQUE_LABELS_N)\n",
        "model = nn.Sequential(nn.Linear(UNIQUE_WORDS_N, 200),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(200, UNIQUE_LABELS_N),\n",
        "                      )\n",
        "\n",
        "scheduler = lambda optim: \\\n",
        "    torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=4, factor=0.5, verbose=True)\n",
        "\n",
        "best_val_loss, best_model = train_eval_loop(model=model,\n",
        "                                            train_dataset=train_dataset,\n",
        "                                            val_dataset=test_dataset,\n",
        "                                            criterion=F.cross_entropy,\n",
        "                                            lr=1e-3,\n",
        "                                            epoch_n=200,\n",
        "                                            batch_size=32,\n",
        "                                            l2_reg_alpha=0.000001, # regularization with weight_decay\n",
        "                                            lr_scheduler_ctor=scheduler)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Эпоха 0\n",
            "Эпоха: 354 итераций, 5.44 сек\n",
            "Среднее значение функции потерь на обучении 1.4429776775887457\n",
            "Среднее значение функции потерь на валидации 0.7668132766828699\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 1\n",
            "Эпоха: 354 итераций, 5.51 сек\n",
            "Среднее значение функции потерь на обучении 0.191704234539789\n",
            "Среднее значение функции потерь на валидации 0.5891784994657767\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 2\n",
            "Эпоха: 354 итераций, 5.40 сек\n",
            "Среднее значение функции потерь на обучении 0.04720906951690966\n",
            "Среднее значение функции потерь на валидации 0.5574984402095867\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 3\n",
            "Эпоха: 354 итераций, 5.41 сек\n",
            "Среднее значение функции потерь на обучении 0.01986088356566168\n",
            "Среднее значение функции потерь на валидации 0.5482129007893599\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 4\n",
            "Эпоха: 354 итераций, 5.40 сек\n",
            "Среднее значение функции потерь на обучении 0.011915894058838193\n",
            "Среднее значение функции потерь на валидации 0.5435718682168399\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 5\n",
            "Эпоха: 354 итераций, 5.49 сек\n",
            "Среднее значение функции потерь на обучении 0.008431382329297899\n",
            "Среднее значение функции потерь на валидации 0.5425672300349352\n",
            "Новая лучшая модель!\n",
            "\n",
            "Эпоха 6\n",
            "Эпоха: 354 итераций, 5.49 сек\n",
            "Среднее значение функции потерь на обучении 0.006653255886611355\n",
            "Среднее значение функции потерь на валидации 0.5473811474265688\n",
            "\n",
            "Эпоха 7\n",
            "Эпоха: 354 итераций, 5.51 сек\n",
            "Среднее значение функции потерь на обучении 0.005876287828478436\n",
            "Среднее значение функции потерь на валидации 0.5436199845083184\n",
            "\n",
            "Эпоха 8\n",
            "Эпоха: 354 итераций, 5.38 сек\n",
            "Среднее значение функции потерь на обучении 0.005707792773327164\n",
            "Среднее значение функции потерь на валидации 0.5475253211492199\n",
            "\n",
            "Эпоха 9\n",
            "Эпоха: 354 итераций, 5.42 сек\n",
            "Среднее значение функции потерь на обучении 0.004518995406652016\n",
            "Среднее значение функции потерь на валидации 0.5493191832644959\n",
            "\n",
            "Эпоха 10\n",
            "Эпоха: 354 итераций, 5.42 сек\n",
            "Среднее значение функции потерь на обучении 0.003972808882728282\n",
            "Среднее значение функции потерь на валидации 0.5485941402424696\n",
            "Epoch    11: reducing learning rate of group 0 to 5.0000e-04.\n",
            "\n",
            "Эпоха 11\n",
            "Эпоха: 354 итераций, 5.43 сек\n",
            "Среднее значение функции потерь на обучении 0.0033417566778732853\n",
            "Среднее значение функции потерь на валидации 0.5478030551161807\n",
            "\n",
            "Эпоха 12\n",
            "Эпоха: 354 итераций, 5.47 сек\n",
            "Среднее значение функции потерь на обучении 0.002902668909666965\n",
            "Среднее значение функции потерь на валидации 0.5483905522752617\n",
            "\n",
            "Эпоха 13\n",
            "Эпоха: 354 итераций, 5.43 сек\n",
            "Среднее значение функции потерь на обучении 0.0029127587104547706\n",
            "Среднее значение функции потерь на валидации 0.5502485747496455\n",
            "\n",
            "Эпоха 14\n",
            "Эпоха: 354 итераций, 5.39 сек\n",
            "Среднее значение функции потерь на обучении 0.0024326235081574294\n",
            "Среднее значение функции потерь на валидации 0.5479207070133949\n",
            "\n",
            "Эпоха 15\n",
            "Эпоха: 354 итераций, 5.57 сек\n",
            "Среднее значение функции потерь на обучении 0.002544679988266395\n",
            "Среднее значение функции потерь на валидации 0.5506439003462004\n",
            "Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n",
            "\n",
            "Эпоха 16\n",
            "Эпоха: 354 итераций, 5.49 сек\n",
            "Среднее значение функции потерь на обучении 0.0022498019270488392\n",
            "Среднее значение функции потерь на валидации 0.5510594761472637\n",
            "Модель не улучшилась за последние 10 эпох, прекращаем обучение\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pjro-AYE8HX-",
        "outputId": "80a50cdf-cc3f-4612-b9c2-ba54f8f306e9"
      },
      "source": [
        "train_pred = predict_with_model(best_model, train_dataset)\n",
        "\n",
        "train_loss = F.cross_entropy(torch.from_numpy(train_pred),\n",
        "                             torch.from_numpy(train_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "test_pred = predict_with_model(best_model, test_dataset)\n",
        "\n",
        "test_loss = F.cross_entropy(torch.from_numpy(test_pred),\n",
        "                            torch.from_numpy(test_source['target']).long())\n",
        "\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], test_pred.argmax(-1)))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 354/353.5625 [00:03<00:00, 97.05it/s] \n",
            "  5%|▍         | 11/235.375 [00:00<00:02, 100.10it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на обучении 0.0048853205516934395\n",
            "Доля верных ответов 0.9993812975075128\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "236it [00:02, 97.04it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Среднее значение функции потерь на валидации 0.548416793346405\n",
            "Доля верных ответов 0.8436006372809347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEuxXmUOCk6T"
      },
      "source": [
        "Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "Ig9K2QRwCkS_",
        "outputId": "c8a2621a-1a37-4ad9-d3b3-2ac1e2e082c0"
      },
      "source": [
        "sklearn_pipeline = Pipeline((('vect', TfidfVectorizer(tokenizer=tokenize_text_simple_regex,\n",
        "                                                      # tokenizer=tokenize_text_stemmed,\n",
        "                                                      max_df=MAX_DF,\n",
        "                                                      min_df=MIN_COUNT,\n",
        "                                                      sublinear_tf=True)),\n",
        "                             ('cls', LogisticRegression())))\n",
        "sklearn_pipeline.fit(train_source['data'], train_source['target'])\n",
        "\n",
        "sklearn_train_pred = sklearn_pipeline.predict_proba(train_source['data'])\n",
        "sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                 torch.from_numpy(train_source['target']))\n",
        "print('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\n",
        "print('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\n",
        "print()\n",
        "\n",
        "sklearn_test_pred = sklearn_pipeline.predict_proba(test_source['data'])\n",
        "sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                torch.from_numpy(test_source['target']))\n",
        "print('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\n",
        "print('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-6d3d79ab9e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# sklearn_pipeline.fit(train_source['data'], train_source['target'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msklearn_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_source\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n\u001b[1;32m     11\u001b[0m                                                  torch.from_numpy(train_source['target']))\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn_pipeline' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUQxyzT7A8mX",
        "outputId": "3295a3a5-c4a3-444c-98dc-2e470bac7992"
      },
      "source": [
        "MAX_DF = 0.6\n",
        "# MAX_DF = 0.8\n",
        "MIN_COUNT = 6\n",
        "vectorizer = TfidfVectorizer(tokenizer=tokenize_text_lemmatized,\n",
        "                             max_df=MAX_DF,\n",
        "                             min_df=MIN_COUNT,\n",
        "                             sublinear_tf=True,\n",
        "                             stop_words=stopwords_en,\n",
        "                             ngram_range=(1, 3),\n",
        "                             norm=None\n",
        "                             )\n",
        "train_vectors = vectorizer.fit_transform(train_source['data'])\n",
        "print(train_vectors.shape)\n",
        "test_vectors = vectorizer.transform(test_source['data'])\n",
        "\n",
        "sc = Normalizer()\n",
        "train_vectors = sc.fit_transform(train_vectors)\n",
        "test_vectors = sc.transform(test_vectors)\n",
        "\n",
        "# # for c_ in 2**(np.arange(1, 10, 0.5)):\n",
        "for c_ in [10, 100, 1000]:\n",
        "    print(c_)\n",
        "    cls = LogisticRegression(penalty='l2', C=c_, solver='liblinear')\n",
        "    cls.fit(train_vectors, train_source['target'])\n",
        "\n",
        "    sklearn_train_pred = cls.predict_proba(train_vectors)\n",
        "    sklearn_train_loss = F.cross_entropy(torch.from_numpy(sklearn_train_pred),\n",
        "                                                    torch.from_numpy(train_source['target']))\n",
        "    print('Среднее значение функции потерь на обучении', float(sklearn_train_loss))\n",
        "    print('Доля верных ответов', accuracy_score(train_source['target'], sklearn_train_pred.argmax(-1)))\n",
        "    print()\n",
        "\n",
        "    sklearn_test_pred = cls.predict_proba(test_vectors)\n",
        "    sklearn_test_loss = F.cross_entropy(torch.from_numpy(sklearn_test_pred),\n",
        "                                                    torch.from_numpy(test_source['target']))\n",
        "    print('Среднее значение функции потерь на валидации', float(sklearn_test_loss))\n",
        "    print('Доля верных ответов', accuracy_score(test_source['target'], sklearn_test_pred.argmax(-1)))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(11314, 55648)\n",
            "10\n",
            "Среднее значение функции потерь на обучении 2.218400819892674\n",
            "Доля верных ответов 0.998585822874315\n",
            "\n",
            "Среднее значение функции потерь на валидации 2.429922401471365\n",
            "Доля верных ответов 0.844397238449283\n",
            "100\n",
            "Среднее значение функции потерь на обучении 2.103933866158613\n",
            "Доля верных ответов 0.9996464557185788\n",
            "\n",
            "Среднее значение функции потерь на валидации 2.307891177322554\n",
            "Доля верных ответов 0.8414763674986724\n",
            "1000\n",
            "Среднее значение функции потерь на обучении 2.0823929587507206\n",
            "Доля верных ответов 0.999734841788934\n",
            "\n",
            "Среднее значение функции потерь на валидации 2.268346028074309\n",
            "Доля верных ответов 0.8421402018056293\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osBcGMjQYEb_"
      },
      "source": [
        "### Report\n",
        "\n",
        "Сначала добавила регуляризацию (через weight_decay, просто передав 0.01, 0.001, 0.0001, ... коэффициенты в дефолтную функцию) -- модель вообще почти перестала обучаться, т.к. коэффициенты видимо были для нее слишком жесткими. При этом совсем маленькие коэффициенты типа 0.0000001 уже почти ничего не улучшали. Я не поняла, что это было, т.к. регуляризация с C = 10, 100, 1000 в sklearn.LogisticRegression на тех же фичах заметно улучшала предсказания.\n",
        "\n",
        "Использование для обучения признаков, подготовленных с помощью TfidfVectorizer, сильно улучшило результаты. С признаками из TfidfVectorizer и на pytorch модели, и в обычной LogisticRegression при правильно подобранных коэффициентах регуляризации получаются результаты лучше, чем в Pipeline с той же LogisticRegression из курса, где коэффициенты не подбираются. Сначала я подумала, что это из-за других шкалирования/нормализациии данных, но дело оказалось в формулах и для TF, и для IDF.\n",
        "\n",
        "Сделала лемматизацию и стемминг -- качество предсказаний ухудшилось. Словарь ожидаемо уменьшился. Я ожидала, что переобучение уменьшится, но как-то не очень.\n",
        "\n",
        "Потом пошла по порядку.\n",
        "\n",
        "1) Заменила tfidf на ltfidf (sublinear_tf=True в sklearn). В pytorch модели это ожидаемо не помогло, т.к. она переобучается, в sklearn улучшило на 2% результаты на валидации).\n",
        "Изменение MAX_DF и MIN_COUNT не очень сильно улучшают все, разве что MIN_COUNT = 6 немного лучше, чем 5.\n",
        "\n",
        "2) PMI к лейблу мне вообще не понравилось использовать. Во-первых, в курсе очень странно дано определение PMI. По-моему, оно не совсем правильное. Если использовать максимально приближенное к нему определение PMI для расчета, то получается, что для частых слов, которые хотя бы раз используются в тексте, PMI одинаковая?..\n",
        "\n",
        "3) Разные нормировки в применении к изначальным взвешенным через TF-IDF данным. \n",
        "Нормировка на std трейна каждой фичи не помогла. Mean, чтобы сохранить разреженность, не вычитала. \n",
        "Нормировка каждого документа на его норму тоже не помогла.\n",
        "\n",
        "4) Изменение формул для TF и IDF дало 5% улучшение до 80%+.\n",
        "В курсе дана формула IDF = n_docs / word_frequencies, но в TfidfVectorizer и в других местах используется log(n_docs / word_frequencies) + 1, или же log((n_docs + 1) / (word_frequencies + 1)) + 1 для стабильности. TF в TfidfVectorizer не нормируется на размер документа. С этой формулой + нормализацией каждого вектора к 1 + 0.000001 регуляризацией я получила примерно те же результаты, что и с использованием TfidfVectorizer + регуляризацией -- 83.2%. С обычной логистической регрессией из sklearn с C=10 получилось 84%, что на 2-3% лучше, чем у примера из курса, завернутого в Pipeline. Снова попробовала сделать нормировку на std и на максимум, и она не помогла.\n",
        "\n",
        "5) N-граммы с лемматизацией / стеммингом.\n",
        "Ожидаемо очень сильно увеличивали размерность и ухудшали результаты, если не подстраивать под них остальные параметры. \n",
        "n-граммы без включения исходных слов ухудшали результаты с разными параметрами.\n",
        "Если сильно уменьшить допустимый интервал между MAX_DF и MIN_COUNTS, то точность с [1, 2] и [1, 2, 3]-граммами слегка улучшилась по сравнению с полученной на (5). Для [1, 2, 3] я вообще взяла MAX_DF=0.2. Лемматизация / стемминг примерно одинаковый результат дают. Выкидывание частых слов из nltk списка stopwords перед созданием n-грамм выгодно уменьшало размерность. Если использовать stopwords, то MAX_DF можно снова сделать побольше, типа 0.6 -- разницы почти нет. Тут лучший результат был 84.2%.\n",
        "\n",
        "6) Усложнение сетки. Добавила слой с сигмоидой, пробовала от 15, 50, 100, 200 нейронов. С lr=0.1 и 0.001 вышло как-то грустненько: почти всегда результаты ухудшались; вариант со 100 нейронами работал нормально при условии наличия регуляризации, но результаты не улучшал. Затем изменила lr на 0.0001, который подсмотрела в комментариях, -- и наконец-то получилось чуть-чуть лучше обычной регрессии. Тут лучший результат был 84.6%.\n"
      ]
    }
  ]
}