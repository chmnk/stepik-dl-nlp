{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "task2_word_embeddings.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMc3yxSBqhVK"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "As-j6-x4qhVN",
        "outputId": "a8c44a25-1296-497a-b9bd-2b2c5aa5b441",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 12.95 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.7.0+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/39/6f/86bd5d0eaa6821ba9193bbed16b660ea6f342fe63ec2e4fa2c61377bb44b/pyconll-2.3.3-py3-none-any.whl\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/dd/112bd4258cee11e0baaaba064060eb156475a42362e59e3ff28e7ca2d29d/gensim-3.8.1-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 40.6MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/08/1884157a3de72d41fa97cacacafaa49abf00eba53cb7e08615b2b65b4a9d/livelossplot-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.19.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.0.0)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 29.5MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 39.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (0.8)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (51.0.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: requests>=2.21 in /usr/local/lib/python3.6/dist-packages (from pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.23.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (4.0.1)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.1.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (20.0.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.7.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.21->pyconll->-r stepik-dl-nlp/requirements.txt (line 15)) (1.24.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.0.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (20.8)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.11.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.0)\n",
            "Building wheels for collected packages: wget, ufal.udpipe, intervaltree\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=b7079a387529340be33cd6ff42b31e911cb6a0038e45e9a308f5cb743e965a16\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp36-cp36m-linux_x86_64.whl size=5625175 sha256=2be0df469fbefe01817ba417186c61d037c1a19d1b0c7c8e0b0458a8f7bf81d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26102 sha256=0ea3cc8d2a4e57dbc778ff9c18e6b8aaae364041ce63c6da586a1583181a78d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built wget ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, pymorphy2-dicts-ru, dawg-python, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll, gensim, wget, livelossplot\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pyconll-2.3.3 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:30.785285Z",
          "start_time": "2019-10-29T19:19:29.542846Z"
        },
        "id": "XCcIKhy9qhVO"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, texts_to_token_ids, \\\n",
        "    PaddedSequenceDataset, Embeddings\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "from dlnlputils.visualization import plot_vectors\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mcv1-TJ1Sf4"
      },
      "source": [
        "import sys\n",
        "import ast\n",
        "import numpy as np\n",
        "import time\n",
        "import scipy.sparse\n",
        "from itertools import permutations\n",
        "from itertools import combinations\n"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKGuWXjlqweh"
      },
      "source": [
        "## Разные задания"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uh3Y81bk1DFo"
      },
      "source": [
        "sliding window"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qxzpERtq0T1"
      },
      "source": [
        "\n",
        "\n",
        "def generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=True, list_compr=False, allocfirst=False):\n",
        "    \"\"\"\n",
        "    text - list of integer numbers - ids of tokens in text\n",
        "    window_size - odd integer - width of window\n",
        "    vocab_size - positive integer - number of tokens in vocabulary\n",
        "    ns_rate - positive integer - number of negative tokens to sample per one positive sample\n",
        "\n",
        "    returns list of training samples (CenterWord, CtxWord, Label)\n",
        "    \"\"\"\n",
        "    halfstep = window_size // 2\n",
        "    tstart = time.perf_counter()\n",
        "    n_txt = len(text)\n",
        "    if naive:\n",
        "        res = []\n",
        "        for i in range(n_txt):\n",
        "            for j in range(-halfstep, halfstep + 1):\n",
        "                if halfstep != 0 and j == 0:\n",
        "                    continue\n",
        "                if i + j < 0 or i + j >= n_txt:\n",
        "                    continue\n",
        "                res.append([text[i], text[i + j], 1])\n",
        "                for k in range(ns_rate):\n",
        "                    res.append([text[i], np.random.randint(0, vocab_size), 0])\n",
        "    elif list_compr:\n",
        "        res = [[text[i], text[i + j], 1] if k == -1 else [text[i], np.random.randint(0, vocab_size), 0] \n",
        "                                           for i in range(n_txt)\n",
        "                                           for j in range(-halfstep, halfstep + 1) if ((i + j >= 0 and i + j < n_txt) and ((j != 0 and halfstep > 0) or (halfstep == 0)))\n",
        "                                           for k in range(-1, ns_rate) \n",
        "            ]\n",
        "    elif not allocfirst:\n",
        "        ndx = [[i, i + j] for i in range(n_txt) for j in range(-halfstep, halfstep + 1) if ((i + j >= 0 and i + j < n_txt) and ((j != 0 and halfstep > 0) or (halfstep == 0)))]\n",
        "        n_pos = len(ndx)\n",
        "        ndx = np.ravel(ndx)\n",
        "        res = text[ndx].reshape(n_pos, 2)\n",
        "        res = np.append(res, np.ones((n_pos, 1), dtype=int), axis=1)\n",
        "        for ineg in range(ns_rate):\n",
        "            res = np.append(res, res[:, :1], axis=1)\n",
        "            res = np.append(res, np.random.randint(0, vocab_size, (n_pos, 1)), axis=1)\n",
        "            res = np.append(res, np.zeros((n_pos, 1), dtype=int), axis=1)\n",
        "        res = res.reshape((n_pos * (ns_rate + 1) , 3))\n",
        "\n",
        "    else:\n",
        "        ndx = [[i, i + j] for i in range(n_txt) for j in range(-halfstep, halfstep + 1) if ((i + j >= 0 and i + j < n_txt) and ((j != 0 and halfstep > 0) or (halfstep == 0)))]\n",
        "        n_pos = len(ndx)\n",
        "        ndx = np.ravel(ndx)\n",
        "        res = np.zeros((n_pos, (ns_rate + 1) * 3), dtype=int)\n",
        "        res[:, :2] = text[ndx].reshape(n_pos, 2)\n",
        "        res[:, 2] = 1\n",
        "        for ineg in range(ns_rate):\n",
        "            ineg3 = 3 + ineg * 3\n",
        "            res[:, ineg3] = res[:, 0]\n",
        "            res[:, ineg3 + 1] = np.random.randint(0, vocab_size, n_pos)\n",
        "            res[:, ineg3 + 2] = 0\n",
        "        res = res.reshape((n_pos * (ns_rate + 1) , 3))\n",
        "\n",
        "    tstop = time.perf_counter()\n",
        "\n",
        "        \n",
        "    return res, tstop - tstart\n",
        "\n"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l9JVp7S1H8_",
        "outputId": "bc362c68-2e53-41be-92e1-8c6f556c99ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "text = np.array([1, 0, 1, 0, 0, 5, 0, 3, 5, 5, 3, 0, 5, 0, 5, 2, 0, 1, 3])\n",
        "text = np.array([ 1,  6,  1,  0, -7])\n",
        "window_size = 3\n",
        "vocab_size = 4\n",
        "ns_rate = 1\n",
        "\n",
        "result, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
        "print(result, tm)\n",
        "\n",
        "result3, tm =  generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False, list_compr=True)\n",
        "print(result3, tm)\n",
        "\n",
        "result1, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False)\n",
        "print(result1.tolist(), tm)\n",
        "\n",
        "result2, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False, allocfirst=True)\n",
        "print(result2.tolist(), tm)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 6, 1], [1, 0, 0], [6, 1, 1], [6, 0, 0], [6, 1, 1], [6, 2, 0], [1, 6, 1], [1, 2, 0], [1, 0, 1], [1, 3, 0], [0, 1, 1], [0, 1, 0], [0, -7, 1], [0, 0, 0], [-7, 0, 1], [-7, 3, 0]] 0.00010782399840536527\n",
            "[[1, 6, 1], [1, 3, 0], [6, 1, 1], [6, 3, 0], [6, 1, 1], [6, 2, 0], [1, 6, 1], [1, 0, 0], [1, 0, 1], [1, 2, 0], [0, 1, 1], [0, 1, 0], [0, -7, 1], [0, 1, 0], [-7, 0, 1], [-7, 1, 0]] 6.450500040955376e-05\n",
            "[[1, 6, 1], [1, 0, 0], [6, 1, 1], [6, 3, 0], [6, 1, 1], [6, 3, 0], [1, 6, 1], [1, 1, 0], [1, 0, 1], [1, 1, 0], [0, 1, 1], [0, 1, 0], [0, -7, 1], [0, 3, 0], [-7, 0, 1], [-7, 2, 0]] 0.0008063140012382064\n",
            "[[1, 6, 1], [1, 2, 0], [6, 1, 1], [6, 0, 0], [6, 1, 1], [6, 3, 0], [1, 6, 1], [1, 2, 0], [1, 0, 1], [1, 0, 0], [0, 1, 1], [0, 0, 0], [0, -7, 1], [0, 1, 0], [-7, 0, 1], [-7, 0, 0]] 0.0004552020000119228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOfAKrfNcur",
        "outputId": "b8d092da-b1a5-4696-dc2c-452a423423fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conditions = ((10, 20, 3, 2),\n",
        "              (10, 20, 3, 10),\n",
        "              (1000, 1000, 3, 2),\n",
        "              (1000, 1000, 11, 2),\n",
        "              (10000, 10000, 5, 5),\n",
        "              (100000, 100000, 5, 3),\n",
        "              (1000000, 1000000, 5, 3),\n",
        "              )\n",
        "\n",
        "\n",
        "for text_size, vocab_size, window_size, ns_rate in conditions:\n",
        "    tms1, tms2, tms3, tms4 = [], [], [], []\n",
        "    text = np.random.randint(0, vocab_size, text_size)\n",
        "\n",
        "    for i in range(10):\n",
        "        result, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate)\n",
        "        tms1.append(tm)\n",
        "\n",
        "        result, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False, list_compr=True)\n",
        "        tms2.append(tm)\n",
        "\n",
        "        result, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False)\n",
        "        tms3.append(tm)\n",
        "\n",
        "        result, tm = generate_w2v_sgns_samples(text, window_size, vocab_size, ns_rate, naive=False, allocfirst=True)\n",
        "        tms4.append(tm)\n",
        "    print('text_size = {0}, vocab_size = {1}, window_size = {2}, ns_rate = {3}'.format(text_size, vocab_size, window_size, ns_rate))\n",
        "    print('naive with 3 for loops              ', np.mean(tms1))\n",
        "    print('naive with list comprehensions      ', np.mean(tms2))\n",
        "    print('vector-style with np.append         ', np.mean(tms3))\n",
        "    print('vector-style with preallocated array', np.mean(tms4))\n",
        "    print()"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text_size = 10, vocab_size = 20, window_size = 3, ns_rate = 2\n",
            "naive with 3 for loops               0.0001862765995610971\n",
            "naive with list comprehensions       0.00017768639991118108\n",
            "vector-style with np.append          0.00014846030026092195\n",
            "vector-style with preallocated array 8.90219998836983e-05\n",
            "\n",
            "text_size = 10, vocab_size = 20, window_size = 3, ns_rate = 10\n",
            "naive with 3 for loops               0.0007047139002679615\n",
            "naive with list comprehensions       0.0006360382998536806\n",
            "vector-style with np.append          0.0006021432003763038\n",
            "vector-style with preallocated array 0.0004060101000504801\n",
            "\n",
            "text_size = 1000, vocab_size = 1000, window_size = 3, ns_rate = 2\n",
            "naive with 3 for loops               0.01615346100043098\n",
            "naive with list comprehensions       0.015966681799909566\n",
            "vector-style with np.append          0.003151491800235817\n",
            "vector-style with preallocated array 0.0030787818996032\n",
            "\n",
            "text_size = 1000, vocab_size = 1000, window_size = 11, ns_rate = 2\n",
            "naive with 3 for loops               0.10676034919997619\n",
            "naive with list comprehensions       0.09527915410035348\n",
            "vector-style with np.append          0.022934380100195995\n",
            "vector-style with preallocated array 0.030481816999599686\n",
            "\n",
            "text_size = 10000, vocab_size = 10000, window_size = 5, ns_rate = 5\n",
            "naive with 3 for loops               0.9841468220998649\n",
            "naive with list comprehensions       0.9871020565995423\n",
            "vector-style with np.append          0.08309952080016955\n",
            "vector-style with preallocated array 0.07921807919956336\n",
            "\n",
            "text_size = 100000, vocab_size = 100000, window_size = 5, ns_rate = 3\n",
            "naive with 3 for loops               5.847728771200491\n",
            "naive with list comprehensions       5.50189137480047\n",
            "vector-style with np.append          0.7656499129996519\n",
            "vector-style with preallocated array 0.8662272374007444\n",
            "\n",
            "text_size = 1000000, vocab_size = 1000000, window_size = 5, ns_rate = 3\n",
            "naive with 3 for loops               54.707623978799894\n",
            "naive with list comprehensions       54.03140085769992\n",
            "vector-style with np.append          7.6576484594999785\n",
            "vector-style with preallocated array 6.7738180060994635\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIRiTJDKiT_x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljG3ZFNViUdi"
      },
      "source": [
        "### GloVe mutual occurence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6NDhX6x1arL",
        "outputId": "721284ea-9f0f-4109-e8e8-b72d1e30d250",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# from itertools import permutations\n",
        "# from itertools import combinations\n",
        "\n",
        "def generate_coocurrence_matrix(texts, vocab_size, naive=True, naive2=False, perm=False, comb_set=False):\n",
        "    \"\"\"\n",
        "    texts - list of lists of ints - i-th sublist contains identifiers of tokens in i-th document\n",
        "    vocab_size - int - size of vocabulary\n",
        "    returns scipy.sparse.dok_matrix\n",
        "    \"\"\"\n",
        "    tstart = time.perf_counter()\n",
        "    mutual_occurence = scipy.sparse.dok_matrix((vocab_size, vocab_size), dtype=int)\n",
        "    if naive:\n",
        "        for text in texts:\n",
        "            dk_loc = scipy.sparse.dok_matrix((vocab_size, vocab_size), dtype=int)\n",
        "            for i, vi in enumerate(text):\n",
        "                for j, vj in enumerate(text):\n",
        "                    if i <= j or vi == vj:\n",
        "                        continue\n",
        "                    dk_loc[vi, vj] = 1\n",
        "                    dk_loc[vj, vi] = 1\n",
        "            mutual_occurence += dk_loc\n",
        "    elif naive2:\n",
        "        for i in range(vocab_size):\n",
        "            for j in range(vocab_size):\n",
        "                if i <= j:\n",
        "                    continue\n",
        "                for text in texts:\n",
        "                    if i in text and j in text:\n",
        "                        mutual_occurence[i, j] += 1\n",
        "                        mutual_occurence[j, i] += 1\n",
        "    elif perm:\n",
        "        for text in texts:\n",
        "            cur_comb = set(permutations(text, 2))\n",
        "            for vi, vj in cur_comb:\n",
        "                if vi == vj:\n",
        "                    continue\n",
        "                mutual_occurence[vi, vj] += 1\n",
        "    elif comb_set:\n",
        "        for text in texts:\n",
        "            combs = combinations(set(text), 2)\n",
        "            for vi, vj in combs:\n",
        "                mutual_occurence[vi, vj] += 1\n",
        "                mutual_occurence[vj, vi] += 1\n",
        "    tstop = time.perf_counter()\n",
        "    return mutual_occurence, tstop - tstart\n",
        "\n",
        "\n",
        "text = [[0, 2, 2, 2, 0, 0], [1, 1, 2, 1, 1], [2, 2, 1, 1]]\n",
        "vocab_size = 3\n",
        "\n",
        "result, tm = generate_coocurrence_matrix(text, vocab_size)\n",
        "print(result.A)\n",
        "result, tm = generate_coocurrence_matrix(text, vocab_size, naive=False, perm=True)\n",
        "print(result.A)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 1]\n",
            " [0 0 2]\n",
            " [1 2 0]]\n",
            "[[0 0 1]\n",
            " [0 0 2]\n",
            " [1 2 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAdJfpten1SU",
        "outputId": "26d8c343-e6a9-41d5-920d-04da293e6bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "conditions = ((10, 10, 10),\n",
        "              (10, 1000, 10),\n",
        "              (1000, 10, 10),\n",
        "              (10, 10, 1000),\n",
        "              (10, 10, 10000),\n",
        "              (100, 1000, 1000),\n",
        "              (100, 100, 1000),\n",
        "              (10, 10000, 10000),\n",
        "              (100, 1000, 100000),\n",
        "              )\n",
        "\n",
        "for n_texts, text_len, vocab_size in conditions:\n",
        "    tms1, tms2, tms3, tms4 = [], [], [], []\n",
        "    text = np.random.randint(0, vocab_size, (n_texts, text_len))\n",
        "    for i in range(10):\n",
        "        res, tm1 = generate_coocurrence_matrix(text, vocab_size)\n",
        "        tms1.append(tm1)\n",
        "        res, tm2 = generate_coocurrence_matrix(text, vocab_size, naive=False, naive2=True)\n",
        "        tms2.append(tm2)\n",
        "        res, tm3 = generate_coocurrence_matrix(text, vocab_size, naive=False, perm=True)\n",
        "        tms3.append(tm3)\n",
        "        res, tm4 = generate_coocurrence_matrix(text, vocab_size, naive=False, comb_set=True)\n",
        "        tms4.append(tm4)\n",
        "    print('Number of texts = {0}, text size = {1}, vocabulary size = {2}'.format(n_texts, text_len, vocab_size))\n",
        "    print('Naive iteration, iterating over texts and words inside of them', np.mean(tms1))\n",
        "    print('Naive iteration, iterating over words and then over texts', np.mean(tms2))\n",
        "    print('With itertools.permutations', np.mean(tms3))\n",
        "    print('With itertools.combinations over a set', np.mean(tms4))\n",
        "    print('')\n",
        "\n"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of texts = 10, text size = 10, vocabulary size = 10\n",
            "Naive iteration, iterating over texts and words inside of them 0.024581806000787763\n",
            "Naive iteration, iterating over words and then over texts 0.01212527020034031\n",
            "With itertools.permutations 0.007330570699559757\n",
            "With itertools.combinations over a set 0.007139380598528078\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-168-0bd1e10cb422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_coocurrence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_coocurrence_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaive2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-4f0d95122ead>\u001b[0m in \u001b[0;36mgenerate_coocurrence_matrix\u001b[0;34m(texts, vocab_size, naive, naive2, perm, comb_set)\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mdk_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mdk_loc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mmutual_occurence\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdk_loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnaive2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m_validate_indices\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpack_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0misintlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mM\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/sparse/sputils.py\u001b[0m in \u001b[0;36misintlike\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;31m# Fast-path check to eliminate non-scalar values. operator.index would\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;31m# catch this case too, but the exception catching is slow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL2yxvOkqhVP"
      },
      "source": [
        "## Загрузка данных и подготовка корпуса"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:31.270503Z",
          "start_time": "2019-10-29T19:19:30.787789Z"
        },
        "id": "1cp3q3zAqhVQ",
        "outputId": "00061013-4b8f-4683-d0ff-47badb7a6dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "full_dataset = list(pd.read_csv('./stepik-dl-nlp/datasets/nyt-ingredients-snapshot-2015.csv')['input'].dropna())\n",
        "random.shuffle(full_dataset)\n",
        "\n",
        "TRAIN_VAL_SPLIT = int(len(full_dataset) * 0.7)\n",
        "train_source = full_dataset[:TRAIN_VAL_SPLIT]\n",
        "test_source = full_dataset[TRAIN_VAL_SPLIT:]\n",
        "print(\"Обучающая выборка\", len(train_source))\n",
        "print(\"Тестовая выборка\", len(test_source))\n",
        "print()\n",
        "print('\\n'.join(train_source[:10]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Обучающая выборка 125344\n",
            "Тестовая выборка 53719\n",
            "\n",
            "1/4 cup sour cream\n",
            "10 ounces swordfish, red snapper or other firm-fleshed fish\n",
            "1 tablespoon minced basil leaves\n",
            "Handful fresh parsley, finely minced\n",
            "4 ounces lard or butter, plus more for brushing tops\n",
            "4 to 5 green cardamom pods\n",
            "1 stick ( 1/4 pound) unsalted butter, softened\n",
            "1/4 teaspoon red pepper flakes, preferably Turkish or Aleppo (see note), more to taste\n",
            "1 tablespoon fresh lemon juice\n",
            "1/4 cup scallions, thinly sliced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.137838Z",
          "start_time": "2019-10-29T19:19:31.272363Z"
        },
        "id": "PCFQ7jeRqhVQ"
      },
      "source": [
        "# токенизируем\n",
        "train_tokenized = tokenize_corpus(train_source)\n",
        "test_tokenized = tokenize_corpus(test_source)\n",
        "print('\\n'.join(' '.join(sent) for sent in train_tokenized[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.325205Z",
          "start_time": "2019-10-29T19:19:32.140837Z"
        },
        "id": "tqvBXk0_qhVR"
      },
      "source": [
        "# строим словарь\n",
        "vocabulary, word_doc_freq = build_vocabulary(train_tokenized, max_doc_freq=0.9, min_count=5, pad_word='<PAD>')\n",
        "print(\"Размер словаря\", len(vocabulary))\n",
        "print(list(vocabulary.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.686258Z",
          "start_time": "2019-10-29T19:19:32.327711Z"
        },
        "id": "SMmkf6ddqhVS"
      },
      "source": [
        "# отображаем в номера токенов\n",
        "train_token_ids = texts_to_token_ids(train_tokenized, vocabulary)\n",
        "test_token_ids = texts_to_token_ids(test_tokenized, vocabulary)\n",
        "\n",
        "print('\\n'.join(' '.join(str(t) for t in sent)\n",
        "                for sent in train_token_ids[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:32.967989Z",
          "start_time": "2019-10-29T19:19:32.688319Z"
        },
        "id": "sxR-yPnqqhVT"
      },
      "source": [
        "plt.hist([len(s) for s in train_token_ids], bins=20);\n",
        "plt.title('Гистограмма длин предложений');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.001487Z",
          "start_time": "2019-10-29T19:19:32.970153Z"
        },
        "id": "i_RhdjNCqhVT"
      },
      "source": [
        "MAX_SENTENCE_LEN = 20\n",
        "train_dataset = PaddedSequenceDataset(train_token_ids,\n",
        "                                      np.zeros(len(train_token_ids)),\n",
        "                                      out_len=MAX_SENTENCE_LEN)\n",
        "test_dataset = PaddedSequenceDataset(test_token_ids,\n",
        "                                     np.zeros(len(test_token_ids)),\n",
        "                                     out_len=MAX_SENTENCE_LEN)\n",
        "print(train_dataset[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1FCqeDAqhVU"
      },
      "source": [
        "## Алгоритм обучения - Skip Gram Negative Sampling\n",
        "\n",
        "**Skip Gram** - предсказываем соседние слова по центральному слову\n",
        "\n",
        "**Negative Sampling** - аппроксимация softmax\n",
        "\n",
        "$$ W, D \\in \\mathbb{R}^{Vocab \\times EmbSize} $$\n",
        "\n",
        "$$ \\sum_{CenterW_i} P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) \\rightarrow \\max_{W,D} $$\n",
        "\n",
        "$$ P(CtxW_{-2}, CtxW_{-1}, CtxW_{+1}, CtxW_{+2} | CenterW_i; W, D) = \\prod_j P(CtxW_j | CenterW_i; W, D) $$\n",
        "    \n",
        "$$ P(CtxW_j | CenterW_i; W, D) = \\frac{e^{w_i \\cdot d_j}} { \\sum_{j=1}^{|V|} e^{w_i \\cdot d_j}} = softmax \\simeq \\frac{e^{w_i \\cdot d_j^+}} { \\sum_{j=1}^{k} e^{w_i \\cdot d_j^-}}, \\quad k \\ll |V| $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.065376Z",
          "start_time": "2019-10-29T19:19:33.003081Z"
        },
        "id": "dpMkisZbqhVW"
      },
      "source": [
        "def make_diag_mask(size, radius):\n",
        "    \"\"\"Квадратная матрица размера Size x Size с двумя полосами ширины radius вдоль главной диагонали\"\"\"\n",
        "    idxs = torch.arange(size)\n",
        "    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n",
        "    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n",
        "    return mask\n",
        "\n",
        "make_diag_mask(10, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETIIXu2pqhVW"
      },
      "source": [
        "**Negative Sampling** работает следующим образом - мы **максимизируем сумму вероятностей двух событий**: \n",
        "\n",
        "* \"этот пример центрального слова вместе с контекстными словами взят **из тренировочной выборки**\": $$ P(y=1 | CenterW_i; CtxW_j) = sigmoid(w_i \\cdot d_j) = \\frac{1}{1+e^{-w_i \\cdot d_j}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "* \"этот пример центрального слова вместе со случайми контекстными словами **выдуман** \": $$ P(y=0 | CenterW_i; CtxW_{noise}) = 1 - P(y=1 | CenterW_i;  CtxW_{noise}) = \\frac{1}{1+e^{w_i \\cdot d_{noise}}} $$\n",
        "\n",
        "$$ \\\\ $$\n",
        "\n",
        "$$ NEG(CtxW_j, CenterW_i) = log(\\frac{1}{1+e^{-w_i \\cdot d_j}}) + \\sum_{l=1}^{k}log(\\frac{1}{1+e^{w_i \\cdot d_{noise_l}}})  \\rightarrow \\max_{W,D} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.101379Z",
          "start_time": "2019-10-29T19:19:33.068154Z"
        },
        "id": "E1I0-KP-qhVW"
      },
      "source": [
        "class SkipGramNegativeSamplingTrainer(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_size, sentence_len, radius=5, negative_samples_n=5):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.negative_samples_n = negative_samples_n\n",
        "\n",
        "        self.center_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)\n",
        "        self.center_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.center_emb.weight.data[0] = 0\n",
        "\n",
        "        self.context_emb = nn.Embedding(self.vocab_size, emb_size, padding_idx=0)        \n",
        "        self.context_emb.weight.data.uniform_(-1.0 / emb_size, 1.0 / emb_size)\n",
        "        self.context_emb.weight.data[0] = 0\n",
        "\n",
        "        self.positive_sim_mask = make_diag_mask(sentence_len, radius)\n",
        "    \n",
        "    def forward(self, sentences):\n",
        "        \"\"\"sentences - Batch x MaxSentLength - идентификаторы токенов\"\"\"\n",
        "        batch_size = sentences.shape[0]\n",
        "        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n",
        "\n",
        "        # оценить сходство с настоящими соседними словами\n",
        "        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n",
        "        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n",
        "        positive_probs = torch.sigmoid(positive_sims)\n",
        "\n",
        "        # увеличить оценку вероятности встретить эти пары слов вместе\n",
        "        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n",
        "        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n",
        "                                               positive_mask.expand_as(positive_probs))\n",
        "\n",
        "        # выбрать случайные \"отрицательные\" слова\n",
        "        negative_words = torch.randint(1, self.vocab_size,\n",
        "                                       size=(batch_size, self.negative_samples_n),\n",
        "                                       device=sentences.device)  # Batch x NegSamplesN\n",
        "        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n",
        "        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n",
        "        \n",
        "        # уменьшить оценку вероятность встретить эти пары слов вместе\n",
        "        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n",
        "                                                           negative_sims.new_zeros(negative_sims.shape))\n",
        "\n",
        "        return positive_loss + negative_loss\n",
        "\n",
        "\n",
        "def no_loss(pred, target):\n",
        "    \"\"\"Фиктивная функция потерь - когда модель сама считает функцию потерь\"\"\"\n",
        "    return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVUxfpcfqhVX"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:19:33.130307Z",
          "start_time": "2019-10-29T19:19:33.103036Z"
        },
        "id": "McB_19JSqhVX"
      },
      "source": [
        "trainer = SkipGramNegativeSamplingTrainer(len(vocabulary), 100, MAX_SENTENCE_LEN,\n",
        "                                          radius=5, negative_samples_n=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.830221Z",
          "start_time": "2019-10-29T19:19:33.132062Z"
        },
        "scrolled": false,
        "id": "eGw0wPf1qhVX"
      },
      "source": [
        "best_val_loss, best_model = train_eval_loop(trainer,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            no_loss,\n",
        "                                            lr=1e-2,\n",
        "                                            epoch_n=2,\n",
        "                                            batch_size=8,\n",
        "                                            device='cpu',\n",
        "                                            early_stopping_patience=10,\n",
        "                                            max_batches_per_epoch_train=2000,\n",
        "                                            max_batches_per_epoch_val=len(test_dataset),\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.862018Z",
          "start_time": "2019-10-29T19:20:12.832046Z"
        },
        "id": "ZyrMCatCqhVY"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(trainer.state_dict(), 'models/sgns.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.888270Z",
          "start_time": "2019-10-29T19:20:12.864706Z"
        },
        "id": "mfG2VRLLqhVZ"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "trainer.load_state_dict(torch.load('models/sgns.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX-KtNt7qhVZ"
      },
      "source": [
        "## Исследуем характеристики полученных векторов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.919904Z",
          "start_time": "2019-10-29T19:20:12.890671Z"
        },
        "id": "imz9KjomqhVZ"
      },
      "source": [
        "embeddings = Embeddings(trainer.center_emb.weight.detach().cpu().numpy(), vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.942708Z",
          "start_time": "2019-10-29T19:20:12.921619Z"
        },
        "id": "W49DZuCPqhVZ"
      },
      "source": [
        "embeddings.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.965936Z",
          "start_time": "2019-10-29T19:20:12.944423Z"
        },
        "id": "4uZoamcpqhVZ"
      },
      "source": [
        "embeddings.analogy('cake', 'cacao', 'cheese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:12.991060Z",
          "start_time": "2019-10-29T19:20:12.967532Z"
        },
        "id": "qotnZhwVqhVa"
      },
      "source": [
        "test_words = ['salad', 'fish', 'salmon', 'sauvignon', 'beef', 'pork', 'steak', 'beer', 'cake', 'coffee', 'sausage', 'wine', 'merlot', 'zinfandel', 'trout', 'chardonnay', 'champagne', 'cacao']\n",
        "test_vectors = embeddings.get_vectors(*test_words)\n",
        "print(test_vectors.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.318676Z",
          "start_time": "2019-10-29T19:20:12.996595Z"
        },
        "id": "m5ZlkHhdqhVa"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(test_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxoh6qMeqhVb"
      },
      "source": [
        "## Обучение Word2Vec с помощью Gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:13.613797Z",
          "start_time": "2019-10-29T19:20:13.321353Z"
        },
        "id": "wUGv1ckjqhVb"
      },
      "source": [
        "import gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.075005Z",
          "start_time": "2019-10-29T19:20:13.615729Z"
        },
        "id": "Y52A2qibqhVb"
      },
      "source": [
        "word2vec = gensim.models.Word2Vec(sentences=train_tokenized, size=100,\n",
        "                                  window=5, min_count=5, workers=4,\n",
        "                                  sg=1, iter=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.109583Z",
          "start_time": "2019-10-29T19:20:17.076599Z"
        },
        "id": "W__aRSBuqhVb"
      },
      "source": [
        "word2vec.wv.most_similar('chicken')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.176357Z",
          "start_time": "2019-10-29T19:20:17.112948Z"
        },
        "id": "r27ivOpSqhVc"
      },
      "source": [
        "gensim_words = [w for w in test_words if w in word2vec.wv.vocab]\n",
        "gensim_vectors = np.stack([word2vec.wv[w] for w in gensim_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.428874Z",
          "start_time": "2019-10-29T19:20:17.179311Z"
        },
        "id": "gpxgKnVrqhVd"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(gensim_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TI0DvCwqhVd"
      },
      "source": [
        "## Загрузка предобученного Word2Vec\n",
        "\n",
        "Источники готовых векторов:\n",
        "\n",
        "https://rusvectores.org/ru/ - для русского языка\n",
        "\n",
        "https://wikipedia2vec.github.io/wikipedia2vec/pretrained/ - много разных языков"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.460133Z",
          "start_time": "2019-10-29T19:20:17.430563Z"
        },
        "id": "Mi48BvB5qhVd"
      },
      "source": [
        "import gensim.downloader as api"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:20:17.980509Z",
          "start_time": "2019-10-29T19:20:17.462239Z"
        },
        "id": "XUOlESxuqhVd"
      },
      "source": [
        "available_models = api.info()['models'].keys()\n",
        "print('\\n'.join(available_models))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.649035Z",
          "start_time": "2019-10-29T19:20:17.984118Z"
        },
        "scrolled": false,
        "id": "hbAzf9cjqhVd"
      },
      "source": [
        "pretrained = api.load('word2vec-google-news-300')  # > 1.5 GB!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.651388Z",
          "start_time": "2019-10-29T19:19:29.817Z"
        },
        "id": "n_Hy6RXWqhVd"
      },
      "source": [
        "pretrained.most_similar('cheese')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.652649Z",
          "start_time": "2019-10-29T19:19:29.820Z"
        },
        "id": "x7FcWhecqhVe"
      },
      "source": [
        "pretrained.most_similar(positive=['man', 'queen'], negative=['king'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.653584Z",
          "start_time": "2019-10-29T19:19:29.823Z"
        },
        "id": "_-2YIdbHqhVe"
      },
      "source": [
        "pretrained_words = [w for w in test_words if w in pretrained.vocab]\n",
        "pretrained_vectors = np.stack([pretrained[w] for w in pretrained_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:22:12.654594Z",
          "start_time": "2019-10-29T19:19:29.828Z"
        },
        "id": "cA8ZTYzyqhVe"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "fig.set_size_inches((10, 10))\n",
        "plot_vectors(pretrained_vectors, test_words, how='svd', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpveHd_GqhVe"
      },
      "source": [
        "## Заключение\n",
        "\n",
        "* Реализовали Skip Gram Negative Sampling на PyTorch\n",
        "* Обучили на корпусе рецептов\n",
        "    * Сходство слов модель выучила неплохо\n",
        "    * Для аналогий мало данных\n",
        "* Обучили SGNS с помощью библиотеки Gensim\n",
        "* Загрузили веса Word2Vec, полученные с помощью большого корпуса (GoogleNews)\n",
        "    * Списки похожих слов отличаются!\n",
        "    * Аналогии работают"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVkFcey0qhVf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}