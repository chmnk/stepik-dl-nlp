{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "colab": {
      "name": "task3_cnn_postag.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbc6oXwZks0F"
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5K035b0nqWR"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KtOwtunntwe",
        "outputId": "21403617-b244-493d-85cc-6d23928ba013",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x = torch.tensor([[1, 1, 0, 0, 1], [0, 1, 0, 1, 0]]).T\n",
        "print(x.shape)\n",
        "krn = torch.tensor([[1, 1, 0], [0, 1, 1]])\n",
        "print(krn.shape)\n",
        "b = 0\n",
        "K = krn.shape[1]\n",
        "n_in, n_ch = x.shape\n",
        "n_out = n_in - K + 1\n",
        "y = torch.zeros(n_out)\n",
        "for pos in range(n_out):\n",
        "    cur_v = 0\n",
        "    for k in range(0, K):\n",
        "        cur_v += torch.dot(x[pos + k], krn[:, k])\n",
        "    y[pos] = b + cur_v\n",
        "    \n",
        "krn = torch.tensor([[[1, 1, 0], [0, 1, 1]], [[1, 0, 0], [0, 0, 1]]])\n",
        "print(krn.shape)\n",
        "K1, K = krn.shape[0], krn.shape[2]\n",
        "n_in, n_ch = x.shape\n",
        "n_out = n_in - K + 1\n",
        "y = torch.zeros((n_out, K1))\n",
        "b = torch.zeros(K1)\n",
        "for pos in range(n_out):\n",
        "    cur_v = torch.tensordot(x[pos : pos + K, :], krn, dims=([0, 1], [2, 1]))\n",
        "    y[pos] = b + cur_v\n",
        "print(y.numpy())"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 2])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 2, 3])\n",
            "[[3. 1.]\n",
            " [2. 2.]\n",
            " [1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQCXyl2Saa5v",
        "outputId": "fdcf350f-b0b2-429c-d8b0-a33ec5d782cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "a = torch.tensor([1.,2.,3.])\n",
        "b = torch.tensor([[-1.,-2.,-3.], [0.1, 0.2, 0.1]]).T\n",
        "print(torch.mm(torch.unsqueeze(a, 0), b))\n",
        "print(torch.dot(a, b[:, 0]))\n",
        "print(torch.dot(a, b[:, 1]))\n"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-14.0000,   0.8000]])\n",
            "tensor(-14.)\n",
            "tensor(0.8000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ws4Hrp4RjTQ7"
      },
      "source": [
        "# Свёрточные нейросети и POS-теггинг\n",
        "\n",
        "POS-теггинг - определение частей речи (снятие частеречной неоднозначности)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bTZv5X9jTRO",
        "outputId": "60db39b7-bcaa-4c86-9e81-190732ea4ced",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle,\n",
        "# выполните следующие строчки, чтобы подгрузить библиотеку dlnlputils:\n",
        "\n",
        "!git clone https://github.com/Samsung-IT-Academy/stepik-dl-nlp.git && pip install -r stepik-dl-nlp/requirements.txt\n",
        "import sys; sys.path.append('./stepik-dl-nlp')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'stepik-dl-nlp'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 289 (delta 10), reused 14 (delta 6), pack-reused 266\u001b[K\n",
            "Receiving objects: 100% (289/289), 42.27 MiB | 21.54 MiB/s, done.\n",
            "Resolving deltas: 100% (139/139), done.\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 1)) (0.22.2.post1)\n",
            "Collecting spacy-udpipe\n",
            "  Downloading https://files.pythonhosted.org/packages/16/60/2a985e25f6a398655f018e5e43d16ba3dbd65f0d4d6ae22add90578669a5/spacy_udpipe-0.3.2-py3-none-any.whl\n",
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 4)) (1.8.1+cu101)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 5)) (3.2.2)\n",
            "Collecting ipymarkup\n",
            "  Downloading https://files.pythonhosted.org/packages/bf/9b/bf54c98d50735a4a7c84c71e92c5361730c878ebfe903d2c2d196ef66055/ipymarkup-0.9.0-py3-none-any.whl\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 7)) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 8)) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 9)) (1.1.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 10)) (4.41.1)\n",
            "Collecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/1c/224cdc3d9a32ed706c8fb1f30b491be6ea5da114ff4edc174014cc24fa43/youtokentome-1.0.6-cp37-cp37m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 12)) (0.11.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 13)) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from -r stepik-dl-nlp/requirements.txt (line 14)) (5.5.0)\n",
            "Collecting pyconll\n",
            "  Downloading https://files.pythonhosted.org/packages/60/7f/148a5b6f99b8a22373bfbcafd9d6776278fec14810ae95c4fe37965f6619/pyconll-3.0.4-py3-none-any.whl\n",
            "Collecting gensim==3.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2MB 1.9MB/s \n",
            "\u001b[?25hCollecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Collecting livelossplot==0.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0f/08/1884157a3de72d41fa97cacacafaa49abf00eba53cb7e08615b2b65b4a9d/livelossplot-0.5.3-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r stepik-dl-nlp/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.2.4)\n",
            "Collecting ufal.udpipe>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/72/2b8b9dc7c80017c790bb3308bbad34b57accfed2ac2f1f4ab252ff4e9cb2/ufal.udpipe-1.2.0.3.tar.gz (304kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->-r stepik-dl-nlp/requirements.txt (line 3)) (0.6.2)\n",
            "Collecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 44.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->-r stepik-dl-nlp/requirements.txt (line 4)) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r stepik-dl-nlp/requirements.txt (line 5)) (1.3.1)\n",
            "Collecting intervaltree>=3\n",
            "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->-r stepik-dl-nlp/requirements.txt (line 9)) (2018.9)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from youtokentome->-r stepik-dl-nlp/requirements.txt (line 11)) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (54.2.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.8.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1->-r stepik-dl-nlp/requirements.txt (line 16)) (5.0.0)\n",
            "Requirement already satisfied: bokeh; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.3.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.1.3)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup->-r stepik-dl-nlp/requirements.txt (line 6)) (2.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (4.7.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (22.0.3)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.1.0->ipykernel->-r stepik-dl-nlp/requirements.txt (line 13)) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->-r stepik-dl-nlp/requirements.txt (line 14)) (0.2.5)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (2.11.3)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (20.9)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (3.13)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.10.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (2020.12.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.7->bokeh; python_version >= \"3.6\"->livelossplot==0.5.3->-r stepik-dl-nlp/requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy-udpipe->-r stepik-dl-nlp/requirements.txt (line 2)) (3.4.1)\n",
            "Building wheels for collected packages: wget, ufal.udpipe, intervaltree\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp37-none-any.whl size=9681 sha256=b89f5de5555771afcc4711078d78e35d42ff444e5429c0aeccb33270d25bdcef\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for ufal.udpipe (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ufal.udpipe: filename=ufal.udpipe-1.2.0.3-cp37-cp37m-linux_x86_64.whl size=5626614 sha256=4ca7eccf8998681c6dc2da6e10a6c49301a86687186eec93e9fe5f211decda40\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/9d/db/6d3404c33da5b7adb6c6972853efb6a27649d3ba15f7e9bebb\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26102 sha256=f1344cb816174d1035965b802780662426df0210c3f46c921190b0533b40ae25\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/f2/66/e9c30d3e9499e65ea2fa0d07c002e64de63bd0adaa49c445bf\n",
            "Successfully built wget ufal.udpipe intervaltree\n",
            "Installing collected packages: ufal.udpipe, spacy-udpipe, dawg-python, pymorphy2-dicts-ru, pymorphy2, intervaltree, ipymarkup, youtokentome, pyconll, gensim, wget, livelossplot\n",
            "  Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed dawg-python-0.7.2 gensim-3.8.1 intervaltree-3.1.0 ipymarkup-0.9.0 livelossplot-0.5.3 pyconll-3.0.4 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 spacy-udpipe-0.3.2 ufal.udpipe-1.2.0.3 wget-3.2 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:42:57.976431Z",
          "start_time": "2019-10-29T19:42:57.959538Z"
        },
        "id": "j629aoU5jTRS",
        "outputId": "9da95b1d-f4d1-43f5-dcbb-951256a29750",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install pyconll\n",
        "!pip install spacy_udpipe"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyconll in /usr/local/lib/python3.7/dist-packages (3.0.4)\n",
            "Requirement already satisfied: spacy_udpipe in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "Requirement already satisfied: ufal.udpipe>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (1.2.0.3)\n",
            "Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy_udpipe) (2.2.4)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.4.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.19.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (54.2.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.0.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->spacy_udpipe) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->spacy_udpipe) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:34.549739Z",
          "start_time": "2019-10-29T19:49:32.179692Z"
        },
        "id": "4P05ohGljTRU"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import pyconll\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "import dlnlputils\n",
        "from dlnlputils.data import tokenize_corpus, build_vocabulary, \\\n",
        "    character_tokenize, pos_corpus_to_tensor, POSTagger\n",
        "from dlnlputils.pipeline import train_eval_loop, predict_with_model, init_random_seed\n",
        "\n",
        "init_random_seed()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_z2s_SVjTRV"
      },
      "source": [
        "## Загрузка текстов и разбиение на обучающую и тестовую подвыборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:08.433599Z",
          "start_time": "2019-10-29T19:46:05.110693Z"
        },
        "id": "ES_3qyxMjTRX"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
        "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.525561Z",
          "start_time": "2019-10-29T19:49:37.315213Z"
        },
        "id": "CqX_0_aEjTRZ"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "full_train = pyconll.load_from_file('./datasets/ru_syntagrus-ud-train.conllu')\n",
        "full_test = pyconll.load_from_file('./datasets/ru_syntagrus-ud-dev.conllu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.548127Z",
          "start_time": "2019-10-29T19:49:56.527559Z"
        },
        "id": "f1rSAl9UjTRa"
      },
      "source": [
        "for sent in full_train[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:56.916262Z",
          "start_time": "2019-10-29T19:49:56.549806Z"
        },
        "id": "AEg6xARQjTRc"
      },
      "source": [
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:57.251433Z",
          "start_time": "2019-10-29T19:49:56.919818Z"
        },
        "id": "MEtzeOnijTRd"
      },
      "source": [
        "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
        "print('\\n'.join(all_train_texts[:10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.124148Z",
          "start_time": "2019-10-29T19:49:57.254191Z"
        },
        "id": "R5_E9U0VjTRf"
      },
      "source": [
        "train_char_tokenized = tokenize_corpus(all_train_texts, tokenizer=character_tokenize)\n",
        "char_vocab, word_doc_freq = build_vocabulary(train_char_tokenized, max_doc_freq=1.0, min_count=5, pad_word='<PAD>')\n",
        "print(\"Количество уникальных символов\", len(char_vocab))\n",
        "print(list(char_vocab.items())[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.524125Z",
          "start_time": "2019-10-29T19:49:58.125577Z"
        },
        "id": "pCjVOlc4jTRh"
      },
      "source": [
        "UNIQUE_TAGS = ['<NOTAG>'] + sorted({token.upos for sent in full_train for token in sent if token.upos})\n",
        "label2id = {label: i for i, label in enumerate(UNIQUE_TAGS)}\n",
        "label2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.752672Z",
          "start_time": "2019-10-29T19:49:58.526431Z"
        },
        "id": "ph_C8YLajTRi"
      },
      "source": [
        "train_inputs, train_labels = pos_corpus_to_tensor(full_train, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "\n",
        "test_inputs, test_labels = pos_corpus_to_tensor(full_test, char_vocab, label2id, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.754883Z",
          "start_time": "2019-10-29T19:49:40.582Z"
        },
        "scrolled": true,
        "id": "UxbOQkFnjTRk"
      },
      "source": [
        "train_inputs[1][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:49:58.756496Z",
          "start_time": "2019-10-29T19:49:40.711Z"
        },
        "id": "mG64dqaYjTRk"
      },
      "source": [
        "train_labels[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42OXu3MWjTRl"
      },
      "source": [
        "## Вспомогательная свёрточная архитектура"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.316516Z",
          "start_time": "2019-10-29T19:46:17.539Z"
        },
        "id": "hJNI-fP9jTRm"
      },
      "source": [
        "class StackedConv1d(nn.Module):\n",
        "    def __init__(self, features_num, layers_n=1, kernel_size=3, conv_layer=nn.Conv1d, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for _ in range(layers_n):\n",
        "            layers.append(nn.Sequential(\n",
        "                conv_layer(features_num, features_num, kernel_size, padding=kernel_size//2),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.LeakyReLU()))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"x - BatchSize x FeaturesNum x SequenceLen\"\"\"\n",
        "        for layer in self.layers:\n",
        "            x = x + layer(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y3r1btLjTRm"
      },
      "source": [
        "## Предсказание частей речи на уровне отдельных токенов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.317452Z",
          "start_time": "2019-10-29T19:46:23.135Z"
        },
        "id": "XxEFUU7GjTRm"
      },
      "source": [
        "class SingleTokenPOSTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, labels_num, embedding_size=32, **kwargs):\n",
        "        super().__init__()\n",
        "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.backbone = StackedConv1d(embedding_size, **kwargs)\n",
        "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.out = nn.Linear(embedding_size, labels_num)\n",
        "        self.labels_num = labels_num\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
        "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
        "        \n",
        "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "        \n",
        "        features = self.backbone(char_embeddings)\n",
        "        \n",
        "        global_features = self.global_pooling(features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
        "        \n",
        "        logits_flat = self.out(global_features)  # BatchSize*MaxSentenceLen x LabelsNum\n",
        "        logits = logits_flat.view(batch_size, max_sent_len, self.labels_num)  # BatchSize x MaxSentenceLen x LabelsNum\n",
        "        logits = logits.permute(0, 2, 1)  # BatchSize x LabelsNum x MaxSentenceLen\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.318497Z",
          "start_time": "2019-10-29T19:46:23.764Z"
        },
        "id": "viW36op0jTRo"
      },
      "source": [
        "single_token_model = SingleTokenPOSTagger(len(char_vocab), len(label2id), embedding_size=64, layers_n=3, kernel_size=3, dropout=0.3)\n",
        "print('Количество параметров', sum(np.product(t.shape) for t in single_token_model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.319470Z",
          "start_time": "2019-10-29T19:46:25.552Z"
        },
        "scrolled": false,
        "id": "zUB64i8jjTRp"
      },
      "source": [
        "(best_val_loss,\n",
        " best_single_token_model) = train_eval_loop(single_token_model,\n",
        "                                            train_dataset,\n",
        "                                            test_dataset,\n",
        "                                            F.cross_entropy,\n",
        "                                            lr=5e-3,\n",
        "                                            epoch_n=10,\n",
        "                                            batch_size=64,\n",
        "                                            device='cuda',\n",
        "                                            early_stopping_patience=5,\n",
        "                                            max_batches_per_epoch_train=500,\n",
        "                                            max_batches_per_epoch_val=100,\n",
        "                                            lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                       factor=0.5,\n",
        "                                                                                                                       verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.320568Z",
          "start_time": "2019-10-29T19:46:47.579Z"
        },
        "id": "4MYTMX5JjTRq"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(best_single_token_model.state_dict(), './models/single_token_pos.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.321566Z",
          "start_time": "2019-10-29T19:46:47.731Z"
        },
        "id": "WSpNmLy4jTRq"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "single_token_model.load_state_dict(torch.load('./models/single_token_pos.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.324276Z",
          "start_time": "2019-10-29T19:46:48.445Z"
        },
        "id": "ieN3ODbkjTRr"
      },
      "source": [
        "train_pred = predict_with_model(single_token_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(single_token_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9N3YZwDjTRs"
      },
      "source": [
        "## Предсказание частей речи на уровне предложений (с учётом контекста)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.325744Z",
          "start_time": "2019-10-29T19:46:50.139Z"
        },
        "id": "wVTpEXtSjTRt"
      },
      "source": [
        "class SentenceLevelPOSTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, labels_num, embedding_size=32, single_backbone_kwargs={}, context_backbone_kwargs={}):\n",
        "        super().__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.char_embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
        "        self.single_token_backbone = StackedConv1d(embedding_size, **single_backbone_kwargs)\n",
        "        self.context_backbone = StackedConv1d(embedding_size, **context_backbone_kwargs)\n",
        "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
        "        self.out = nn.Conv1d(embedding_size, labels_num, 1)\n",
        "        self.labels_num = labels_num\n",
        "    \n",
        "    def forward(self, tokens):\n",
        "        \"\"\"tokens - BatchSize x MaxSentenceLen x MaxTokenLen\"\"\"\n",
        "        batch_size, max_sent_len, max_token_len = tokens.shape\n",
        "        tokens_flat = tokens.view(batch_size * max_sent_len, max_token_len)\n",
        "        \n",
        "        char_embeddings = self.char_embeddings(tokens_flat)  # BatchSize*MaxSentenceLen x MaxTokenLen x EmbSize\n",
        "        char_embeddings = char_embeddings.permute(0, 2, 1)  # BatchSize*MaxSentenceLen x EmbSize x MaxTokenLen\n",
        "        char_features = self.single_token_backbone(char_embeddings)\n",
        "        \n",
        "        token_features_flat = self.global_pooling(char_features).squeeze(-1)  # BatchSize*MaxSentenceLen x EmbSize\n",
        "\n",
        "        token_features = token_features_flat.view(batch_size, max_sent_len, self.embedding_size)  # BatchSize x MaxSentenceLen x EmbSize\n",
        "        token_features = token_features.permute(0, 2, 1)  # BatchSize x EmbSize x MaxSentenceLen\n",
        "        context_features = self.context_backbone(token_features)  # BatchSize x EmbSize x MaxSentenceLen\n",
        "\n",
        "        logits = self.out(context_features)  # BatchSize x LabelsNum x MaxSentenceLen\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.326925Z",
          "start_time": "2019-10-29T19:46:50.310Z"
        },
        "id": "PDzFUGaKjTRu"
      },
      "source": [
        "sentence_level_model = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
        "                                              single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3),\n",
        "                                              context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3))\n",
        "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-10-29T19:47:48.327888Z",
          "start_time": "2019-10-29T19:46:50.737Z"
        },
        "scrolled": false,
        "id": "Pq3WyBdnjTRu"
      },
      "source": [
        "(best_val_loss,\n",
        " best_sentence_level_model) = train_eval_loop(sentence_level_model,\n",
        "                                              train_dataset,\n",
        "                                              test_dataset,\n",
        "                                              F.cross_entropy,\n",
        "                                              lr=5e-3,\n",
        "                                              epoch_n=10,\n",
        "                                              batch_size=64,\n",
        "                                              device='cuda',\n",
        "                                              early_stopping_patience=5,\n",
        "                                              max_batches_per_epoch_train=500,\n",
        "                                              max_batches_per_epoch_val=100,\n",
        "                                              lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                         factor=0.5,\n",
        "                                                                                                                         verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:16.542052Z",
          "start_time": "2019-08-29T13:56:16.529110Z"
        },
        "id": "wMzISXKejTRv"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "torch.save(best_sentence_level_model.state_dict(), './models/sentence_level_pos.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:16.564926Z",
          "start_time": "2019-08-29T13:56:16.544481Z"
        },
        "id": "hAHhWiEqjTRw"
      },
      "source": [
        "# Если Вы запускаете ноутбук на colab или kaggle, добавьте в начало пути ./stepik-dl-nlp\n",
        "sentence_level_model.load_state_dict(torch.load('./models/sentence_level_pos.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.092139Z",
          "start_time": "2019-08-29T13:56:16.567242Z"
        },
        "id": "yoBANl9ujTRw"
      },
      "source": [
        "train_pred = predict_with_model(sentence_level_model, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(sentence_level_model, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsAFAFq6jTRy"
      },
      "source": [
        "## Применение полученных теггеров и сравнение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.105418Z",
          "start_time": "2019-08-29T13:56:42.093744Z"
        },
        "id": "VUZrokx7jTRy"
      },
      "source": [
        "single_token_pos_tagger = POSTagger(single_token_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)\n",
        "sentence_level_pos_tagger = POSTagger(sentence_level_model, char_vocab, UNIQUE_TAGS, MAX_SENT_LEN, MAX_ORIG_TOKEN_LEN)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.125540Z",
          "start_time": "2019-08-29T13:56:42.106771Z"
        },
        "id": "8Ce7ELP2jTRz"
      },
      "source": [
        "test_sentences = [\n",
        "    'Мама мыла раму.',\n",
        "    'Косил косой косой косой.',\n",
        "    'Глокая куздра штеко будланула бокра и куздрячит бокрёнка.',\n",
        "    'Сяпала Калуша с Калушатами по напушке.',\n",
        "    'Пирожки поставлены в печь, мама любит печь.',\n",
        "    'Ведро дало течь, вода стала течь.',\n",
        "    'Три да три, будет дырка.',\n",
        "    'Три да три, будет шесть.',\n",
        "    'Сорок сорок'\n",
        "]\n",
        "test_sentences_tokenized = tokenize_corpus(test_sentences, min_token_size=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.148124Z",
          "start_time": "2019-08-29T13:56:42.126930Z"
        },
        "id": "L5ohjHwWjTR0"
      },
      "source": [
        "for sent_tokens, sent_tags in zip(test_sentences_tokenized, single_token_pos_tagger(test_sentences)):\n",
        "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.168810Z",
          "start_time": "2019-08-29T13:56:42.149698Z"
        },
        "id": "DJQTm8gajTR0"
      },
      "source": [
        "for sent_tokens, sent_tags in zip(test_sentences_tokenized, sentence_level_pos_tagger(test_sentences)):\n",
        "    print(' '.join('{}-{}'.format(tok, tag) for tok, tag in zip(sent_tokens, sent_tags)))\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWgDuruKjTR0"
      },
      "source": [
        "## Свёрточный модуль своими руками"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.193140Z",
          "start_time": "2019-08-29T13:56:42.170233Z"
        },
        "id": "3KujWJy3jTR1"
      },
      "source": [
        "class MyConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.weight = nn.Parameter(torch.randn(in_channels * kernel_size, out_channels) / (in_channels * kernel_size),\n",
        "                                   requires_grad=True)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_channels), requires_grad=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"x - BatchSize x InChannels x SequenceLen\"\"\"\n",
        "\n",
        "        batch_size, src_channels, sequence_len = x.shape        \n",
        "        if self.padding > 0:\n",
        "            pad = x.new_zeros(batch_size, src_channels, self.padding)\n",
        "            x = torch.cat((pad, x, pad), dim=-1)\n",
        "            sequence_len = x.shape[-1]\n",
        "\n",
        "        chunks = []\n",
        "        chunk_size = sequence_len - self.kernel_size + 1\n",
        "        for offset in range(self.kernel_size):\n",
        "            chunks.append(x[:, :, offset:offset + chunk_size])\n",
        "\n",
        "        in_features = torch.cat(chunks, dim=1)  # BatchSize x InChannels * KernelSize x ChunkSize\n",
        "        in_features = in_features.permute(0, 2, 1)  # BatchSize x ChunkSize x InChannels * KernelSize\n",
        "        out_features = torch.bmm(in_features, self.weight.unsqueeze(0).expand(batch_size, -1, -1)) + self.bias.unsqueeze(0).unsqueeze(0)\n",
        "        out_features = out_features.permute(0, 2, 1)  # BatchSize x OutChannels x ChunkSize\n",
        "        return out_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T13:56:42.210013Z",
          "start_time": "2019-08-29T13:56:42.194620Z"
        },
        "id": "eKEeQjU9jTR2"
      },
      "source": [
        "sentence_level_model_my_conv = SentenceLevelPOSTagger(len(char_vocab), len(label2id), embedding_size=64,\n",
        "                                                      single_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d),\n",
        "                                                      context_backbone_kwargs=dict(layers_n=3, kernel_size=3, dropout=0.3, conv_layer=MyConv1d))\n",
        "print('Количество параметров', sum(np.product(t.shape) for t in sentence_level_model_my_conv.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T14:06:00.233326Z",
          "start_time": "2019-08-29T13:56:42.211456Z"
        },
        "id": "1pF421DOjTR2"
      },
      "source": [
        "(best_val_loss,\n",
        " best_sentence_level_model_my_conv) = train_eval_loop(sentence_level_model_my_conv,\n",
        "                                                      train_dataset,\n",
        "                                                      test_dataset,\n",
        "                                                      F.cross_entropy,\n",
        "                                                      lr=5e-3,\n",
        "                                                      epoch_n=10,\n",
        "                                                      batch_size=64,\n",
        "                                                      device='cuda',\n",
        "                                                      early_stopping_patience=5,\n",
        "                                                      max_batches_per_epoch_train=500,\n",
        "                                                      max_batches_per_epoch_val=100,\n",
        "                                                      lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=2,\n",
        "                                                                                                                                 factor=0.5,\n",
        "                                                                                                                                 verbose=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-29T14:06:39.145214Z",
          "start_time": "2019-08-29T14:06:00.234936Z"
        },
        "id": "lW76zLsNjTR2"
      },
      "source": [
        "train_pred = predict_with_model(best_sentence_level_model_my_conv, train_dataset)\n",
        "train_loss = F.cross_entropy(torch.tensor(train_pred),\n",
        "                             torch.tensor(train_labels))\n",
        "print('Среднее значение функции потерь на обучении', float(train_loss))\n",
        "print(classification_report(train_labels.view(-1), train_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))\n",
        "print()\n",
        "\n",
        "test_pred = predict_with_model(best_sentence_level_model_my_conv, test_dataset)\n",
        "test_loss = F.cross_entropy(torch.tensor(test_pred),\n",
        "                            torch.tensor(test_labels))\n",
        "print('Среднее значение функции потерь на валидации', float(test_loss))\n",
        "print(classification_report(test_labels.view(-1), test_pred.argmax(1).reshape(-1), target_names=UNIQUE_TAGS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0TBG959npFO"
      },
      "source": [
        ""
      ]
    }
  ]
}